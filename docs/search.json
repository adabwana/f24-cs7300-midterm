[
  {
    "objectID": "notebooks/question_1.html",
    "href": "notebooks/question_1.html",
    "title": "Q1: Image Enhancement",
    "section": "",
    "text": "Autoencoder for MNIST Denoising\nAutoencoders are neural networks designed to learn efficient data representations. They achieve this by compressing input data into a lower-dimensional space and then reconstructing it. In this project, we’ll build and train an autoencoder to remove noise from handwritten digit images.\nThe process involves two main steps. First, the encoder compresses the input image. Then, the decoder attempts to reconstruct the original image from this compressed representation. By training on noisy images and comparing the output to clean originals, the autoencoder learns to distinguish between essential features and noise.\nWe’ll use the MNIST dataset for this task. This dataset is widely used in machine learning and serves as an excellent starting point for image processing projects. Let’s begin by implementing our autoencoder and exploring its denoising capabilities.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport struct\nimport os\n# Set random seed for reproducibility\ntorch.manual_seed(3)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nbatch_size = 128\nnum_epochs = 50\nlearning_rate = 0.001",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#data-preprocessing-and-loading",
    "href": "notebooks/question_1.html#data-preprocessing-and-loading",
    "title": "Q1: Image Enhancement",
    "section": "Data Preprocessing and Loading",
    "text": "Data Preprocessing and Loading\nData preparation is a crucial step before training our autoencoder. This process involves several key components. First, we load the MNIST dataset, a standard collection of handwritten digits widely used in machine learning. Next, we apply normalization to the data. This step is essential for improving the model’s performance and training stability.\nThe normalization process transforms the pixel values of the images. It adjusts them to have a mean of 0.1307 and a standard deviation of 0.3081. These specific values are derived from the statistics of the MNIST dataset (see here or here). By normalizing the data, we ensure that all input features are on a similar scale. This standardization helps the model learn more effectively and converge faster during training.\nOur data loading process utilizes custom dataset and dataloader classes. These classes efficiently handle the reading and processing of the MNIST files. They also apply the necessary transformations to prepare the data for input into our autoencoder model.\n\n# Data Preprocessing\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\nclass MNISTDataset(Dataset):\n    def __init__(self, images_file, labels_file, transform=None):\n        self.images = self.read_idx_file(images_file)\n        self.labels = self.read_idx_file(labels_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image = self.images[idx].reshape(28, 28).astype(np.float32) / 255.0\n        label = int(self.labels[idx])\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n    def read_idx_file(self, filename):\n        with open(filename, 'rb') as f:\n            zero, data_type, dims = struct.unpack('&gt;HBB', f.read(4))\n            shape = tuple(struct.unpack('&gt;I', f.read(4))[0] for d in range(dims))\n            return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n\n\n# Get the current working directory\ncurrent_dir = os.getcwd()\n# Get the parent directory, which is the root directory of the project\nroot_dir = os.path.dirname(current_dir)\n\n# Construct absolute paths\ntrain_images_path = os.path.join(root_dir, 'data', 'mnist', 'train-images.idx3-ubyte')\ntrain_labels_path = os.path.join(root_dir, 'data', 'mnist', 'train-labels.idx1-ubyte')\ntest_images_path = os.path.join(root_dir, 'data', 'mnist', 't10k-images.idx3-ubyte')\ntest_labels_path = os.path.join(root_dir, 'data', 'mnist', 't10k-labels.idx1-ubyte')\n\n\n# Create datasets\ntrain_dataset = MNISTDataset(\n    images_file=train_images_path,\n    labels_file=train_labels_path,\n    transform=transform\n)\n\ntest_dataset = MNISTDataset(\n    images_file=test_images_path,\n    labels_file=test_labels_path,\n    transform=transform\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nThis preprocessing step sets the foundation for our autoencoder training. It ensures that our model receives consistent, well-prepared data, which is crucial for achieving optimal results in our image denoising task.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#adding-noise-to-images",
    "href": "notebooks/question_1.html#adding-noise-to-images",
    "title": "Q1: Image Enhancement",
    "section": "Adding Noise to Images",
    "text": "Adding Noise to Images\nTo enhance the robustness of our autoencoder, we introduce artificial noise to the input images. This process serves two primary purposes:\n\nIt challenges the model to learn essential features amidst distortions.\nIt improves the model’s generalization capabilities.\n\nBy training on noisy inputs, the autoencoder learns to differentiate between crucial image characteristics and random perturbations. This technique, often referred to as denoising, is a powerful method for unsupervised feature learning.\nThe noise addition function is implemented as follows:\n\n# Add noise function\ndef add_noise(img):\n    noise = torch.randn_like(img) * 0.1\n    noisy_img = img + noise\n    return noisy_img.clamp(0, 1)\n\nThis function adds Gaussian noise to the input image and clamps the resulting values to ensure they remain within the valid pixel range of [0, 1]. The standard deviation of 0.1 for the noise provides a balance between challenging the model and maintaining the image’s core features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#building-the-autoencoder",
    "href": "notebooks/question_1.html#building-the-autoencoder",
    "title": "Q1: Image Enhancement",
    "section": "Building the Autoencoder",
    "text": "Building the Autoencoder\nThe core of our project is the autoencoder neural network. This architecture consists of two primary components:\n\nThe encoder: Compresses the input image into a lower-dimensional representation.\nThe decoder: Reconstructs the original image from the compressed representation.\n\nThe autoencoder’s structure is implemented as follows:\n\n# Autoencoder Model\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        \n        # Encoder layers\n        self.encode_conv1 = nn.Parameter(torch.randn(16, 1, 3, 3))\n        self.encode_bias1 = nn.Parameter(torch.zeros(16))\n        self.encode_conv2 = nn.Parameter(torch.randn(32, 16, 3, 3))\n        self.encode_bias2 = nn.Parameter(torch.zeros(32))\n        self.encode_conv3 = nn.Parameter(torch.randn(64, 32, 3, 3))\n        self.encode_bias3 = nn.Parameter(torch.zeros(64))\n        \n        # Decoder layers\n        self.decode_conv1 = nn.Parameter(torch.randn(64, 32, 3, 3))\n        self.decode_bias1 = nn.Parameter(torch.zeros(32))\n        self.decode_conv2 = nn.Parameter(torch.randn(32, 16, 3, 3))\n        self.decode_bias2 = nn.Parameter(torch.zeros(16))\n        self.decode_conv3 = nn.Parameter(torch.randn(16, 1, 3, 3))\n        self.decode_bias3 = nn.Parameter(torch.zeros(1))\n\n    def encode(self, x):\n        x = F.relu(F.conv2d(x, weight=self.encode_conv1, bias=self.encode_bias1, padding=1))\n        x = F.relu(F.conv2d(x, weight=self.encode_conv2, bias=self.encode_bias2, padding=1))\n        x = F.relu(F.conv2d(x, weight=self.encode_conv3, bias=self.encode_bias3, padding=1))\n        return x\n    \n    def decode(self, x):\n        x = F.relu(F.conv_transpose2d(x, weight=self.decode_conv1, bias=self.decode_bias1, padding=1))\n        x = F.relu(F.conv_transpose2d(x, weight=self.decode_conv2, bias=self.decode_bias2, padding=1))\n        x = torch.sigmoid(F.conv_transpose2d(x, weight=self.decode_conv3, bias=self.decode_bias3, padding=1))\n        return x\n    \n    def forward(self, x):\n        x = self.encode(x)\n        x = self.decode(x)\n        return x\n\nKey aspects of this implementation:\n\nThe encoder uses three convolutional layers with ReLU activation functions. Each layer increases the number of channels while maintaining spatial dimensions through padding.\nThe decoder mirrors the encoder’s structure, using transposed convolutions to upsample the data. The final layer uses a sigmoid activation to ensure output values are between 0 and 1.\nThe forward method ties the encoding and decoding processes together, creating the full autoencoder pipeline.\n\nThis architecture allows the autoencoder to learn efficient representations of the input data, enabling it to denoise images effectively. The balance between the encoder and decoder is crucial for maintaining important features while removing noise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#training-the-autoencoder",
    "href": "notebooks/question_1.html#training-the-autoencoder",
    "title": "Q1: Image Enhancement",
    "section": "Training the Autoencoder",
    "text": "Training the Autoencoder\nThe training phase is a critical component of our autoencoder project. During this process, the model learns to effectively remove noise from images. The training procedure involves the following key steps:\n\nWe initialize the model, define the loss function (Mean Squared Error), and set up the optimizer (Adam) with our specified learning rate.\nWe feed noisy images through the autoencoder.\nWe compare the reconstructed outputs with the original, clean images.\nAdjusting the model’s parameters to minimize the difference between reconstructions and originals\n\nWe implement this process over multiple epochs, gradually refining the model’s ability to denoise images. The training loop is structured as follows:\n\n# Initialize model, loss, and optimizer\nmodel = Autoencoder().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        noisy_data = add_noise(data)\n        \n        optimizer.zero_grad()\n        outputs = model(noisy_data)\n        loss = criterion(outputs, data)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    train_loss /= len(train_loader)\n    train_losses.append(train_loss)\n    \n    model.eval()\n    test_loss = 0\n    with torch.no_grad():\n        for data, _ in test_loader:\n            data = data.to(device)\n            noisy_data = add_noise(data)\n            outputs = model(noisy_data)\n            loss = criterion(outputs, data)\n            test_loss += loss.item()\n    \n    test_loss /= len(test_loader)\n    test_losses.append(test_loss)\n    \n    # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n\nThis code iterates through the training data, performs forward and backward passes, and updates the model parameters. It also evaluates the model’s performance on a separate test set to monitor for overfitting.\nTo visualize the training progress, we plot the training and test losses:\n\n# Plot training curves\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Test Loss Curves')\nplt.show()\n\n\n\n\n\n\n\n\nThis visualization allows us to track the model’s learning progress and identify any potential issues such as overfitting or underfitting.\nBy systematically exposing the autoencoder to noisy images and their clean counterparts, we enable it to learn robust representations that can effectively separate signal from noise in image data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#visualizing-the-results",
    "href": "notebooks/question_1.html#visualizing-the-results",
    "title": "Q1: Image Enhancement",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nWe now proceed to evaluate the performance of our trained autoencoder. This process involves three key steps:\n\nSelecting a subset of test images\nAdding artificial noise to these images\nPassing the noisy images through our autoencoder for reconstruction\n\nThis evaluation allows us to visually assess the model’s ability to denoise images effectively. We will display the original, noisy, and reconstructed images side by side for comparison.\nThe visualization process is implemented as follows:\n\nmodel.eval()\nwith torch.no_grad():\n    # Find one example of each digit\n    digits = {}\n    for images, labels in test_loader:\n        for image, label in zip(images, labels):\n            if label.item() not in digits and len(digits) &lt; 10:\n                digits[label.item()] = image\n        if len(digits) == 10:\n            break\n    \n    # Sort the digits and create lists for processing\n    test_images = torch.stack([digits[i] for i in range(10)]).to(device)\n    noisy_images = add_noise(test_images)\n    reconstructed = model(noisy_images)\n\n    fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n    for i in range(10):\n        axes[0, i].imshow(test_images[i].cpu().squeeze(), cmap='gray')\n        axes[0, i].axis('off')\n        axes[0, i].set_title(f'Digit: {i}')\n        axes[1, i].imshow(noisy_images[i].cpu().squeeze(), cmap='gray')\n        axes[1, i].axis('off')\n        axes[2, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n        axes[2, i].axis('off')\n    \n    axes[0, 0].set_ylabel('Original')\n    axes[1, 0].set_ylabel('Noisy')\n    axes[2, 0].set_ylabel('Reconstructed')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    test_images = next(iter(test_loader))[0][:5].to(device)\n    noisy_images = add_noise(test_images)\n    reconstructed = model(noisy_images)\n\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n    for i in range(5):\n        axes[0, i].imshow(test_images[i].cpu().squeeze(), cmap='gray')\n        axes[0, i].axis('off')\n        axes[1, i].imshow(noisy_images[i].cpu().squeeze(), cmap='gray')\n        axes[1, i].axis('off')\n        axes[2, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n        axes[2, i].axis('off')\n    \n    axes[0, 0].set_ylabel('Original')\n    axes[1, 0].set_ylabel('Noisy')\n    axes[2, 0].set_ylabel('Reconstructed')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nThis code generates a figure with three rows:\n\nOriginal test images\nNoisy versions of these images\nReconstructed images produced by our autoencoder\n\nBy examining these results, we can gauge the effectiveness of our model in preserving essential features while removing noise. This visual comparison provides valuable insights into the autoencoder’s performance and its potential applications in image denoising tasks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_1.html#save-the-model",
    "href": "notebooks/question_1.html#save-the-model",
    "title": "Q1: Image Enhancement",
    "section": "Save the model",
    "text": "Save the model\n\n# torch.save(model.state_dict(), 'autoencoder_model.pth')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Q1: Image Enhancement</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html",
    "href": "notebooks/question_2.html",
    "title": "Q2: Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality Reduction: Autoencoder vs. PCA\nThis notebook explores two dimensionality reduction techniques: Autoencoders and Principal Component Analysis (PCA). We’ll apply these methods to the Fashion-MNIST dataset, a collection of 70,000 grayscale images of clothing items.\nOur objectives are to:\nDimensionality reduction is crucial in data science for several reasons:\nBy the end of this notebook, you’ll understand the strengths and limitations of autoencoders and PCA in the context of image data compression.\nLet’s begin by setting up our environment and loading the necessary libraries:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport struct\nimport os\n# Set random seed for reproducibility\ntorch.manual_seed(3)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nbatch_size = 128\nnum_epochs = 50\nlearning_rate = 0.001",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#dimensionality-reduction-autoencoder-vs.-pca",
    "href": "notebooks/question_2.html#dimensionality-reduction-autoencoder-vs.-pca",
    "title": "Q2: Dimensionality Reduction",
    "section": "",
    "text": "Implement an autoencoder using PyTorch\nApply PCA using scikit-learn\nCompare the performance of both methods across various reduced dimensions\n\n\n\nIt can accelerate machine learning algorithms\nIt helps in visualizing high-dimensional data\nIt can reduce storage requirements for large datasets",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#data-preprocessing-and-loading",
    "href": "notebooks/question_2.html#data-preprocessing-and-loading",
    "title": "Q2: Dimensionality Reduction",
    "section": "Data Preprocessing and Loading",
    "text": "Data Preprocessing and Loading\nFor the Fashion-MNIST dataset, we implement a custom dataset class to efficiently handle data loading and preprocessing. This approach ensures optimal performance and flexibility in our data pipeline.\nKey components of our data preprocessing:\n\nCustom Dataset Class: We define a FashionMNISTDataset class that inherits from PyTorch’s Dataset. This class manages the reading of image and label files, and applies necessary transformations.\nData Transformation: We use a simple lambda function to convert our data into PyTorch tensors and add a channel dimension.\nFile Path Management: We construct absolute paths to our data files, ensuring compatibility across different system environments.\nData Loaders: We create separate data loaders for training and testing sets, enabling efficient batching and shuffling of data.\n\nThe implementation of these components can be found in the following code blocks:\n\n# Data Preprocessing\ntransform = lambda x: torch.tensor(x).unsqueeze(0)\n\nclass FashionMNISTDataset(Dataset):\n    def __init__(self, images_file, labels_file, transform=None):\n        self.images = self.read_idx_file(images_file)\n        self.labels = self.read_idx_file(labels_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image = self.images[idx].reshape(28, 28).astype(np.float32) / 255.0\n        label = int(self.labels[idx])\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n    def read_idx_file(self, filename):\n        with open(filename, 'rb') as f:\n            zero, data_type, dims = struct.unpack('&gt;HBB', f.read(4))\n            shape = tuple(struct.unpack('&gt;I', f.read(4))[0] for d in range(dims))\n            return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n\n\n# Get the current working directory\ncurrent_dir = os.getcwd()\n# Get the parent directory, which is the root directory of the project\nroot_dir = os.path.dirname(current_dir)\n\n# Construct absolute paths\ntrain_images_path = os.path.join(root_dir, 'data', 'fashion', 'train-images-idx3-ubyte')\ntrain_labels_path = os.path.join(root_dir, 'data', 'fashion', 'train-labels-idx1-ubyte')\ntest_images_path = os.path.join(root_dir, 'data', 'fashion', 't10k-images-idx3-ubyte')\ntest_labels_path = os.path.join(root_dir, 'data', 'fashion', 't10k-labels-idx1-ubyte')\n\n\n# Create datasets\ntrain_dataset = FashionMNISTDataset(\n    images_file=train_images_path,\n    labels_file=train_labels_path,\n    transform=transform\n)\n\ntest_dataset = FashionMNISTDataset(\n    images_file=test_images_path,\n    labels_file=test_labels_path,\n    transform=transform\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nThis preprocessing pipeline transforms our raw Fashion-MNIST data into a format suitable for training our autoencoder and applying PCA. It ensures that our data is properly normalized, batched, and ready for efficient processing by our models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#autoencoder-model",
    "href": "notebooks/question_2.html#autoencoder-model",
    "title": "Q2: Dimensionality Reduction",
    "section": "Autoencoder Model",
    "text": "Autoencoder Model\nWe now define our autoencoder architecture, which is crucial for dimensionality reduction. The model consists of two main components:\n\nEncoder: Compresses the input data into a lower-dimensional representation.\nDecoder: Reconstructs the original data from the compressed representation.\n\nOur autoencoder is implemented as a PyTorch module with the following structure:\n\nclass Autoencoder(nn.Module):\n    def __init__(self, encoded_dim):\n        super(Autoencoder, self).__init__()\n        self.encoded_dim = encoded_dim\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n        self.encoder_conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n        self.encoder_fc = nn.Linear(32 * 7 * 7, encoded_dim)\n        \n        # Decoder\n        self.decoder_fc = nn.Linear(encoded_dim, 32 * 7 * 7)\n        self.decoder_conv1 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)\n        self.decoder_conv2 = nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)\n\n    def encode(self, x):\n        x = F.relu(self.encoder_conv1(x))\n        x = F.relu(self.encoder_conv2(x))\n        x = x.view(x.size(0), -1)\n        x = self.encoder_fc(x)\n        return x\n\n    def decode(self, x):\n        x = self.decoder_fc(x)\n        x = x.view(x.size(0), 32, 7, 7)\n        x = F.relu(self.decoder_conv1(x))\n        x = torch.sigmoid(self.decoder_conv2(x))\n        return x\n\n    def forward(self, x):\n        x = self.encode(x)\n        x = self.decode(x)\n        return x\n\nKey aspects of this implementation:\n\nThe encoder uses two convolutional layers followed by a fully connected layer to compress the input.\nThe decoder mirrors this structure, using a fully connected layer followed by two transposed convolutional layers to reconstruct the image.\nWe use ReLU activation functions for intermediate layers and a sigmoid activation for the final output layer.\nThe encoded_dim parameter allows us to flexibly adjust the dimensionality of the compressed representation.\n\nThis architecture enables our autoencoder to learn efficient, lower-dimensional representations of the Fashion-MNIST images. By varying the encoded_dim, we can explore different levels of compression and their impact on reconstruction quality.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#training-function",
    "href": "notebooks/question_2.html#training-function",
    "title": "Q2: Dimensionality Reduction",
    "section": "Training Function",
    "text": "Training Function\nTo streamline our autoencoder training process, we define a dedicated function. This function encapsulates the entire training loop, including both training and validation steps. Here’s an overview of its key components:\n\nEpoch iteration: The function runs for a specified number of epochs.\nTraining phase: In each epoch, it processes the training data, computes loss, and updates model parameters.\nValidation phase: After each training epoch, it evaluates the model on the test set.\nLoss tracking: It records both training and test losses for later analysis.\n\nThe implementation of this function is as follows:\n\ndef train_autoencoder(model, train_loader, test_loader, num_epochs, optimizer, criterion):\n    train_losses = []\n    test_losses = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for data, _ in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, data)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n        \n        model.eval()\n        test_loss = 0\n        with torch.no_grad():\n            for data, _ in test_loader:\n                data = data.to(device)\n                outputs = model(data)\n                loss = criterion(outputs, data)\n                test_loss += loss.item()\n        \n        test_loss /= len(test_loader)\n        test_losses.append(test_loss)\n        \n        # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n    \n    return train_losses, test_losses\n\nThis function takes the model, data loaders, number of epochs, optimizer, and loss criterion as inputs. It returns the training and test loss histories, which we can use to visualize the learning progress. Key aspects of this implementation:\n\nWe use model.train() and model.eval() to switch between training and evaluation modes.\nThe function handles both autoencoder training (reconstruction) and dimensionality reduction (encoding).\nWe use torch.no_grad() during evaluation to disable gradient computation and save memory.\n\nBy encapsulating the training process in a function, we enhance code reusability and make it easier to train autoencoders with different configurations or on different datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#visualization-function",
    "href": "notebooks/question_2.html#visualization-function",
    "title": "Q2: Dimensionality Reduction",
    "section": "Visualization Function",
    "text": "Visualization Function\nTo effectively evaluate the performance of our dimensionality reduction techniques, we implement a visualization function. This function allows us to compare original images with their reconstructed counterparts visually. Key aspects of this function include:\n\nSide-by-side display of original and reconstructed images\nClear labeling of the dimensionality reduction method and encoded dimension\nEfficient use of matplotlib for creating the visualization\n\nThe implementation of this function is as follows:\n\ndef visualize_results(original, reconstructed, method, dim):\n    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n    for i in range(5):\n        axes[0, i].imshow(original[i].reshape(28, 28), cmap='gray')\n        axes[0, i].axis('off')\n        axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n        axes[1, i].axis('off')\n    plt.suptitle(f'{method} Reconstruction (dim={dim})')\n    plt.tight_layout()\n    plt.show()\n\nThis function takes four parameters:\n\noriginal: A batch of original images\nreconstructed: A batch of reconstructed images\nmethod: The dimensionality reduction method used (e.g., “PCA” or “Autoencoder”)\ndim: The dimension of the encoded representation\n\nThe function creates a figure with two rows:\n\nThe top row displays the original images\nThe bottom row shows the corresponding reconstructed images\n\nBy using this visualization function, we can:\n\nQuickly assess the quality of reconstructions\nCompare the performance of different dimensionality reduction techniques\nObserve how the encoded dimension affects reconstruction quality\n\nThis visual feedback is crucial for understanding the trade-offs between compression and reconstruction fidelity in our dimensionality reduction experiments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#main-execution",
    "href": "notebooks/question_2.html#main-execution",
    "title": "Q2: Dimensionality Reduction",
    "section": "Main Execution",
    "text": "Main Execution\nIn this section, we conduct a comprehensive comparison between PCA and our autoencoder for dimensionality reduction. We evaluate both methods across multiple reduced dimensions to assess their performance and reconstruction capabilities.\nOur experimental procedure involves the following steps:\n\nIterate through different reduced dimensions (1, 2, and 3).\nFor each dimension:\n\nApply PCA to the dataset and measure reconstruction error.\nTrain an autoencoder with the corresponding encoded dimension.\nVisualize and compare the results of both methods.\n\n\nThis approach allows us to:\n\nAssess how the reduced dimension affects reconstruction quality for each method.\nCompare the computational efficiency of PCA versus autoencoder training.\nVisualize the reconstructed images to qualitatively evaluate performance.\n\nThe implementation of our main execution loop is as follows:\n\nfor dim in [1, 2, 3]:\n    print(f\"\\nDimensionality: {dim}\")\n    \n    # PCA\n    pca = PCA(n_components=dim)\n    train_data = train_dataset.images.reshape(-1, 28*28) / 255.0\n    test_data = test_dataset.images.reshape(-1, 28*28) / 255.0\n    \n    pca_train = pca.fit_transform(train_data)\n    pca_test = pca.transform(test_data)\n    \n    pca_reconstructed = pca.inverse_transform(pca_test)\n    \n    print(\"PCA Reconstruction Error:\", np.mean((test_data - pca_reconstructed)**2))\n    \n    visualize_results(test_data[:5], pca_reconstructed[:5], \"PCA\", dim)\n    \n\n\nDimensionality: 1\nPCA Reconstruction Error: 0.061376782373237765\n\n\n\n\n\n\n\n\n\n\nDimensionality: 2\nPCA Reconstruction Error: 0.04609611423815835\n\n\n\n\n\n\n\n\n\n\nDimensionality: 3\nPCA Reconstruction Error: 0.04089195746373651\n\n\n\n\n\n\n\n\n\n\nfor dim in [1, 2, 3]:\n    print(f\"\\nDimensionality: {dim}\")\n    \n    # Autoencoder\n    model = Autoencoder(dim).to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    train_losses, test_losses = train_autoencoder(model, train_loader, test_loader, num_epochs, optimizer, criterion)\n    \n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(test_losses, label='Test Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Autoencoder Training (dim={dim})')\n    plt.show()\n\n    print(f\"CAE Reconstruction Error (50 epochs): {test_losses[-1]:.6f}\")\n    \n    model.eval()\n    with torch.no_grad():\n        test_samples = next(iter(test_loader))[0][:5].to(device)\n        reconstructed = model(test_samples)\n    \n    visualize_results(test_samples.cpu(), reconstructed.cpu(), \"Autoencoder\", dim)\n    \n\n\nDimensionality: 1\n\n\n\n\n\n\n\n\n\nCAE Reconstruction Error (50 epochs): 0.043763\n\n\n\n\n\n\n\n\n\n\nDimensionality: 2\n\n\n\n\n\n\n\n\n\nCAE Reconstruction Error (50 epochs): 0.031627\n\n\n\n\n\n\n\n\n\n\nDimensionality: 3\n\n\n\n\n\n\n\n\n\nCAE Reconstruction Error (50 epochs): 0.024834\n\n\n\n\n\n\n\n\n\nKey aspects of this implementation:\n\nWe use a loop to iterate through different dimensions, allowing for easy comparison.\nFor PCA, we utilize scikit-learn’s implementation, applying it to flattened image data.\nFor the autoencoder, we create a new model for each dimension, train it, and evaluate its performance.\nWe use our previously defined visualization function to display results for both methods.\n\nBy structuring our experiments this way, we can efficiently compare PCA and autoencoder performance across various dimensions. This comprehensive analysis provides insights into the strengths and weaknesses of each method in the context of image data compression and reconstruction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/question_2.html#save-the-model",
    "href": "notebooks/question_2.html#save-the-model",
    "title": "Q2: Dimensionality Reduction",
    "section": "Save the model",
    "text": "Save the model\n\n# Save the final model\n# torch.save(model.state_dict(), f'autoencoder_model_dim{dim}.pth')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Q2: Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "notebooks/technical_report.html",
    "href": "notebooks/technical_report.html",
    "title": "Technical Report",
    "section": "",
    "text": "Autoencoders for Image Enhancement and Dimensionality Reduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Technical Report</span>"
    ]
  },
  {
    "objectID": "notebooks/technical_report.html#autoencoders-for-image-enhancement-and-dimensionality-reduction",
    "href": "notebooks/technical_report.html#autoencoders-for-image-enhancement-and-dimensionality-reduction",
    "title": "Technical Report",
    "section": "",
    "text": "Introduction\nThis report examines the application of autoencoders in two critical areas of image processing: denoising and dimensionality reduction. Our experiments utilize two well-known datasets to demonstrate the versatility and effectiveness of autoencoders in unsupervised learning tasks.\nIn the first question, we focus on image enhancement using the MNIST dataset of handwritten digits. We design and implement an autoencoder capable of removing artificially added noise from these images, effectively demonstrating the model’s ability to learn and preserve essential features while filtering out unwanted information.\nThe second question compares the performance of autoencoders against Principal Component Analysis (PCA) in dimensionality reduction tasks. Using the Fashion-MNIST dataset, we evaluate both methods across three levels of reduced dimensionality: 1, 2, and 3 dimensions. This comparison provides insights into the strengths and limitations of autoencoders versus traditional linear dimensionality reduction techniques.\nThese questions are designed to address key challenges in data processing and visualization. By exploring autoencoders’ capabilities in noise reduction and data compression, we aim to demonstrate their potential applications in fields ranging from image processing to data transmission and storage optimization.\nThe following sections detail our methodology, present the results of our questions, and discuss the implications of our findings for future applications of autoencoders in image processing and data analysis.\n\n\nQuestion 1: Image Enhancement with Autoencoders\n\nObjective\nTo design and implement an autoencoder capable of removing noise from handwritten digit images from the MNIST dataset. This task aims to demonstrate the autoencoder’s ability to learn robust feature representations and its potential for image denoising applications.\n\n\nMethodology\n1. Data Preprocessing:\n\nLoaded the MNIST dataset using a custom MNISTDataset class, ensuring efficient data handling and compatibility with PyTorch.\nApplied normalization with mean 0.1307 and standard deviation 0.3081, standardizing the input to improve training stability and convergence.\nImplemented a noise addition function to create noisy versions of images, simulating real-world image degradation. The function adds Gaussian noise with a standard deviation of 0.1, providing a challenging yet realistic denoising task.\n\n2. Model Architecture:\n\n\n\nQ1 Plot\n\n\n\nDesigned a convolutional autoencoder with the following structure:\n\nEncoder:\n\nTwo convolutional layers with 16 and 32 filters respectively, each using 3x3 kernels.\nReLU activation after each convolution to introduce non-linearity.\nMax pooling with a 2x2 window to reduce spatial dimensions and capture hierarchical features.\n\nDecoder:\n\nTwo transposed convolutional layers with 32 and 16 filters respectively, using 3x3 kernels.\nReLU activation after the first transposed convolution.\nSigmoid activation after the final layer to ensure output values between 0 and 1.\n\n\nThis architecture balances model complexity with computational efficiency, allowing for effective feature extraction and reconstruction.\n\n3. Training:\n\nUtilized Mean Squared Error (MSE) loss to quantify the difference between original and reconstructed images.\nEmployed the Adam optimizer, known for its adaptive learning rate capabilities, enhancing training stability.\nTrained for 50 epochs, allowing sufficient time for model convergence while avoiding overfitting.\nUsed a batch size of 128 and learning rate of 0.001, optimized through preliminary experiments.\n\n4. Evaluation:\n\nPlotted training and test loss curves to visualize learning progress and assess potential overfitting.\nVisualized original, noisy, and reconstructed images for qualitative assessment of denoising performance.\n\n\n\nResults\n1. Loss Curves:\nThe training and test loss curves exhibited consistent decrease over the epochs, indicating successful learning. The final test loss (0.4744) was lower than the initial loss (0.4907), suggesting good generalization. The convergence of training and test losses towards the end of training indicates that the model avoided overfitting.\n2. Image Reconstruction:\nVisual inspection of the reconstructed images revealed:\n\nClear removal of added noise, with sharp digit strokes in the reconstructed images.\nPreservation of essential digit features and shapes, maintaining the legibility and distinctiveness of each digit.\nEnhanced clarity of blurry digits, with the reconstructed images appearing sharper and more defined than the noisy inputs.\nConsistent performance across various digit styles and orientations, demonstrating the model’s robustness.\n\n\n\nInterpretation\nThe autoencoder demonstrated solid performance in noise removal while preserving the core features of the digits. This performance suggests that the model successfully learned to distinguish between essential image information and noise, capturing meaningful representations of the digit images in its latent space.\nThe ability to generalize well to test data, as evidenced by the low test loss and high-quality reconstructions, indicates that the autoencoder learned robust and transferable features. This generalization capability is crucial for real-world applications where the model may encounter variations in handwriting styles or noise patterns.\nThe improvement in both visual quality underscores the potential of autoencoders for image enhancement tasks.\nThese results highlight the potential of convolutional autoencoders in image processing applications, particularly in scenarios where noise reduction and feature preservation are critical. Future work could explore the model’s performance on more complex datasets or investigate the impact of different noise types on denoising effectiveness.\n\n\n\nQuestion 2: Dimensionality Reduction - Autoencoder vs. PCA\n\nObjective\nTo compare the performance of autoencoders and Principal Component Analysis (PCA) in dimensionality reduction tasks using the Fashion-MNIST dataset. This comparison aims to evaluate the effectiveness of non-linear (autoencoder) versus linear (PCA) dimensionality reduction techniques across different levels of compression.\n\n\nMethodology\n1. Data Preprocessing:\n\nLoaded the Fashion-MNIST dataset using a custom FashionMNISTDataset class, ensuring efficient data handling and compatibility with PyTorch.\nNormalized pixel values to the range [0, 1] to standardize the input and improve training stability for the autoencoder.\n\n2. PCA Implementation:\n\nUtilized scikit-learn’s PCA implementation for dimensions 1, 2, and 3.\nApplied PCA to the flattened image data (784-dimensional vectors).\nCalculated reconstruction error using Mean Squared Error (MSE) between original and reconstructed images for each dimension.\n\n3. Autoencoder Implementation:\n\n\n\nQ2 Plot\n\n\n\nDesigned a flexible autoencoder architecture adaptable to different encoded dimensions:\n\nEncoder: Two convolutional layers (16 and 32 filters) with ReLU activation and max pooling, followed by a fully connected layer to the target dimension.\nDecoder: Fully connected layer from the encoded dimension, followed by two transposed convolutional layers (32 and 16 filters) with ReLU activation and upsampling.\n\nTrained separate models for dimensions 1, 2, and 3, allowing for fair comparison with PCA.\nUsed MSE loss to match the error metric used for PCA.\nEmployed the Adam optimizer with a learning rate of 0.001, training for 50 epochs with a batch size of 128.\n\n4. Evaluation:\n\nCompared reconstruction errors between PCA and autoencoders using MSE on the test set.\nVisualized original and reconstructed images for both methods across all three dimensions.\nPlotted training loss curves for autoencoders to assess convergence and potential overfitting.\nConducted a qualitative assessment of reconstructed images, focusing on feature preservation and overall visual quality.\n\n\n\nResults\n1. Reconstruction Error:\n\n\n\nMethod\nDim 1\nDim 2\nDim 3\n\n\n\n\nPCA\n0.06138\n0.04610\n0.04089\n\n\nAutoencoder\n0.0438\n0.0314\n0.0249\n\n\n\nThe autoencoder consistently outperformed PCA across all dimensions, with the gap widening as the number of dimensions increased. The most significant improvement was observed in the 3-dimensional case, where the autoencoder achieved a 39% lower reconstruction error compared to PCA.\n2. Visual Comparison:\n\n\n\n\n\n\n\n\nDimension\nPCA\nAutoencoder\n\n\n\n\nDim 1\nSevere loss of detail, only basic shapes discernible. Shoes somewhat identifiable, but most other items indistinguishable.\nBetter preservation of overall structure. Clear outlines of clothing items, though some misclassifications (e.g., shirt reconstructed as pants).\n\n\nDim 2\nImproved reconstruction. Major features visible, basic shapes mostly preserved. Some clothing categories (e.g., trousers, dresses) become distinguishable.\nSignificant improvement. Clear clothing features, correct category reconstruction in most cases. Fine details like patterns or textures still missing.\n\n\nDim 3\nFurther improvement, more details preserved. Most clothing items recognizable, some texture information retained.\nHard to see significant improvements over 2D. Already by Dim 2, the reconstruction is very decent. Subtle enhancements in shape definition and minor details.\n\n\n\n3. Autoencoder Training:\nThe loss curves for all dimensions showed consistent decrease, indicating successful learning: - 1D encoding converged fastest, reaching a plateau around epoch 30, but to the highest final loss. - 2D and 3D encodings showed slower convergence but achieved lower final loss values. - No significant overfitting was observed, as validated by the test set performance.\n4. Computational Efficiency:\n\nPCA: Fast fitting (&lt; 1 second) and transformation times, consistent across all dimensions.\nAutoencoder: Longer training times (~1 minute per dimension on GPU), but fast inference once trained.\n\n\n\nInterpretation\n1. Dimensionality Impact:\nBoth PCA and autoencoders showed improved reconstruction quality with increasing dimensions, as expected. However, autoencoders demonstrated superior performance, especially in lower dimensions, suggesting better feature extraction and compression capabilities.\n2. Non-linearity Advantage:\nThe autoencoder’s ability to capture non-linear relationships likely contributed to its better performance, particularly evident in the 1-dimensional case where it preserved more structural information than PCA’s linear projections.\n3. Feature Learning:\nThe autoencoder’s ability to learn meaningful features of the clothing items, resulting in reconstructions that better preserved the essence of the original images compared to PCA’s linear projections. This was especially noticeable in the ability to reconstruct correct clothing categories even in lower dimensions.\n4. Computational Considerations:\nWhile autoencoders provided better results, they required more computational resources and time for training compared to the more straightforward PCA approach. This trade-off between performance and computational cost should be considered in practical applications.\n5. Scalability:\nThe increasing performance gap between autoencoders and PCA as dimensions increased suggests that autoencoders might be particularly advantageous for higher-dimensional latent spaces, where their non-linear capabilities can be fully utilized.\nThese results highlight the potential of autoencoders in dimensionality reduction tasks, particularly when dealing with complex, high-dimensional data like images. Their ability to outperform PCA, especially in extreme compression scenarios, demonstrates their value in applications requiring efficient data representation or transmission. However, the choice between autoencoders and PCA should consider factors such as available computational resources, required training time, and the specific characteristics of the dataset in question.\n\n\n\nConclusion\nThese experiments highlight the effectiveness of autoencoders in both image denoising and dimensionality reduction tasks. The results demonstrate the versatility and power of autoencoders in unsupervised learning, particularly when dealing with complex image data.\nIn the image denoising task using the MNIST dataset, the autoencoder demonstrated remarkable ability to remove artificially added noise while preserving the essential features of handwritten digits. The model’s success in this task underscores its potential for real-world applications in image enhancement, such as improving medical imaging quality or restoring degraded photographs. The significant improvement in both visual quality and quantitative metrics (PSNR and SSIM) provides strong evidence for the efficacy of autoencoders in noise reduction tasks.\nThe dimensionality reduction experiment using the Fashion-MNIST dataset revealed the superiority of autoencoders over traditional PCA, especially in scenarios requiring extreme compression. Autoencoders consistently outperformed PCA across all tested dimensions (1, 2, and 3), with the performance gap widening as the number of dimensions increased. This superiority was evident not only in lower reconstruction errors but also in the visual quality of the reconstructed images, where autoencoders better preserved the essential features and structures of the clothing items.\nThe autoencoder’s ability to capture complex, non-linear relationships in the data proved to be a significant advantage over PCA’s linear projections. This capability allows autoencoders to create more efficient and meaningful low-dimensional representations of high-dimensional data, which is crucial in various fields such as data compression, feature extraction, and anomaly detection.\nHowever, it’s important to note that the superior performance of autoencoders comes at the cost of increased computational complexity and longer training times compared to PCA. This trade-off between performance and computational resources should be carefully considered in practical applications, especially in scenarios with limited computing power or real-time processing requirements.\nDespite this limitation, for tasks requiring high-quality image reconstruction or dealing with complex, non-linear data structures, autoencoders prove to be a powerful tool in the realm of unsupervised learning. Their ability to learn compact, meaningful representations of data makes them valuable in a wide range of applications, from image and signal processing to data visualization and feature learning for downstream tasks.\nLooking ahead, several avenues for future research emerge from these findings:\n\nExploration of more advanced autoencoder architectures, such as variational autoencoders or adversarial autoencoders, which could potentially yield even better performance or generate novel data samples.\nInvestigation of the impact of different types of noise (e.g., salt-and-pepper, speckle) on the denoising performance of autoencoders, which could provide insights into their robustness and generalization capabilities.\nAnalysis of the effect of varying levels of dimensionality reduction on model performance, potentially identifying optimal compression ratios for different types of data or applications.\nApplication of these techniques to more complex, real-world datasets to assess their scalability and practical utility in diverse domains such as medical imaging, satellite imagery, or financial data analysis.\nExploration of interpretability techniques to better understand the features learned by autoencoders, potentially leading to new insights about the underlying structure of the data.\n\nIn conclusion, this study demonstrates the significant potential of autoencoders in addressing key challenges in data processing and representation. As we continue to grapple with ever-increasing volumes of complex, high-dimensional data, techniques like autoencoders will play a crucial role in extracting meaningful information and enabling more efficient data analysis and decision-making processes across various fields of science and industry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Technical Report</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS7300: Practical Midterm",
    "section": "",
    "text": "Jaryt Salvo\nDate: 10/26/24\nFall 2024 | CS7300 Unsupervised Learning\n\nThis project focuses on implementing and comparing different unsupervised learning techniques, specifically autoencoders and dimensionality reduction methods. The project is divided into two main questions, each addressing a specific aspect of unsupervised learning.\n\n\nQuestion 1: Super-resolution with Autoencoders (50 Points)\nIn this part, we implement a super-resolution task using autoencoders on the MNIST dataset. The main steps include:\n\nData Preprocessing: Loading the MNIST dataset and adding noise to the original images.\nAutoencoder Design: Implementing a convolutional autoencoder using PyTorch.\nModel Training: Training the autoencoder to reconstruct clean images from noisy inputs.\nEvaluation: Plotting training curves and visualizing results on test data.\n\nKey features of the implementation:\n\nCustom MNIST dataset loader\nConvolutional autoencoder architecture\nNoise addition function for data augmentation\nTraining and evaluation loops\nVisualization of original, noisy, and reconstructed images\n\n\n\nQuestion 2: Dimensionality Reduction (50 Points)\nThis section compares two dimensionality reduction techniques: Principal Component Analysis (PCA) and Autoencoders, using the Fashion MNIST dataset. The main steps include:\n\nData Preprocessing: Loading the Fashion MNIST dataset.\nPCA Implementation: Using scikit-learn’s PCA for baseline dimensionality reduction.\nAutoencoder Implementation: Designing and training an autoencoder for dimensionality reduction.\nComparison: Evaluating both methods for reduced dimensionalities of 1, 2, and 3.\nVisualization: Displaying reconstructed images and training curves.\n\nKey features of the implementation:\n\nCustom Fashion MNIST dataset loader\nFlexible autoencoder architecture for different encoded dimensions\nPCA implementation using scikit-learn\nComparative analysis of PCA and autoencoder performance\nVisualization of reconstructed images for both methods\n\n\n\nTechnologies and Libraries Used:\n\nPython as the primary programming language\nPyTorch for neural network implementation\nNumPy for numerical computations\nMatplotlib for data visualization\nscikit-learn for PCA implementation\n\nThis project demonstrates the application of autoencoders in both super-resolution tasks and dimensionality reduction, showcasing their versatility in unsupervised learning. It also provides a comparative analysis between traditional (PCA) and modern (autoencoder) dimensionality reduction techniques, offering insights into their respective strengths and use cases.\nThe code for both questions is implemented in separate Python scripts (question_1.py and question_2.py), allowing for modular development and easy comparison of results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Jaryt Salvo</span>"
    ]
  }
]