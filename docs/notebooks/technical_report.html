<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Technical Report – CS7300: Practical Midterm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/question_2.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/technical_report.html"><span class="chapter-title">Technical Report</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">CS7300: Practical Midterm</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Jaryt Salvo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/question_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q1: Image Enhancement</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/question_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Q2: Dimensionality Reduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/technical_report.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Technical Report</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#autoencoders-for-image-enhancement-and-dimensionality-reduction" id="toc-autoencoders-for-image-enhancement-and-dimensionality-reduction" class="nav-link active" data-scroll-target="#autoencoders-for-image-enhancement-and-dimensionality-reduction">Autoencoders for Image Enhancement and Dimensionality Reduction</a>
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#question-1-image-enhancement-with-autoencoders" id="toc-question-1-image-enhancement-with-autoencoders" class="nav-link" data-scroll-target="#question-1-image-enhancement-with-autoencoders"><strong>Question 1</strong>: Image Enhancement with Autoencoders</a>
  <ul class="collapse">
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation">Interpretation</a></li>
  </ul></li>
  <li><a href="#question-2-dimensionality-reduction---autoencoder-vs.-pca" id="toc-question-2-dimensionality-reduction---autoencoder-vs.-pca" class="nav-link" data-scroll-target="#question-2-dimensionality-reduction---autoencoder-vs.-pca"><strong>Question 2</strong>: Dimensionality Reduction - Autoencoder vs.&nbsp;PCA</a>
  <ul class="collapse">
  <li><a href="#objective-1" id="toc-objective-1" class="nav-link" data-scroll-target="#objective-1">Objective</a></li>
  <li><a href="#methodology-1" id="toc-methodology-1" class="nav-link" data-scroll-target="#methodology-1">Methodology</a></li>
  <li><a href="#results-1" id="toc-results-1" class="nav-link" data-scroll-target="#results-1">Results</a></li>
  <li><a href="#interpretation-1" id="toc-interpretation-1" class="nav-link" data-scroll-target="#interpretation-1">Interpretation</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Technical Report</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="autoencoders-for-image-enhancement-and-dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="autoencoders-for-image-enhancement-and-dimensionality-reduction">Autoencoders for Image Enhancement and Dimensionality Reduction</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>This report examines the application of <strong><em>autoencoders</em></strong> in two critical areas of image processing: <em>denoising</em> and <em>dimensionality reduction</em>. Our experiments utilize two well-known datasets to demonstrate the versatility and effectiveness of <strong><em>autoencoders</em></strong> in <em>unsupervised learning tasks</em>.</p>
<p>In the <strong>first question</strong>, we focus on image enhancement using the <code>MNIST</code> dataset of handwritten digits. We design and implement an <strong><em>autoencoder</em></strong> capable of removing artificially added noise from these images, effectively demonstrating the model’s ability to learn and preserve essential features while filtering out unwanted information.</p>
<p>The <strong>second question</strong> compares the performance of <strong><em>autoencoders</em></strong> against <strong><em>Principal Component Analysis (PCA)</em></strong> in <em>dimensionality reduction</em> tasks. Using the <code>Fashion-MNIST</code> dataset, we evaluate both methods across three levels of reduced dimensionality: 1, 2, and 3 dimensions. This comparison provides insights into the strengths and limitations of autoencoders versus traditional linear dimensionality reduction techniques.</p>
<p>These questions are designed to address <em>key challenges</em> in data processing and visualization. By exploring autoencoders’ capabilities in <em>noise reduction</em> and <em>data compression</em>, we aim to demonstrate their potential applications in fields ranging from image processing to <em>data transmission</em> and <em>storage optimization</em>.</p>
<p>The following sections detail our methodology, present the results of our questions, and discuss the implications of our findings for future applications of autoencoders in image processing and data analysis.</p>
</section>
<section id="question-1-image-enhancement-with-autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="question-1-image-enhancement-with-autoencoders"><strong>Question 1</strong>: Image Enhancement with Autoencoders</h3>
<section id="objective" class="level4">
<h4 class="anchored" data-anchor-id="objective">Objective</h4>
<p>To design and implement an <strong><em>autoencoder</em></strong> capable of <em>removing noise</em> from handwritten digit images from the <code>MNIST</code> dataset. This task aims to demonstrate the autoencoder’s ability to <em>learn robust feature representations</em> and its potential for <em>image denoising</em> applications.</p>
</section>
<section id="methodology" class="level4">
<h4 class="anchored" data-anchor-id="methodology">Methodology</h4>
<p><strong>1. Data Preprocessing:</strong></p>
<ul>
<li>Loaded the <code>MNIST</code> dataset using a custom <code>MNISTDataset</code> class, ensuring efficient data handling and compatibility with <code>PyTorch</code>.</li>
<li>Applied <em>normalization</em> with mean 0.1307 and standard deviation 0.3081, standardizing the input to improve training stability and convergence.</li>
<li>Implemented a <em>noise addition function</em> to create noisy versions of images, simulating real-world image degradation. The function adds Gaussian noise with a standard deviation of 0.1, providing a challenging yet realistic <em>denoising task</em>.</li>
</ul>
<p><strong>2. Model Architecture:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/question_1_ae.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Q1 Plot</figcaption>
</figure>
</div>
<ul>
<li><p>Designed a <strong><em>convolutional autoencoder</em></strong> with the following structure:</p>
<ul>
<li><strong>Encoder</strong>:
<ul>
<li><strong><em>Two convolutional layers</em></strong> with 16 and 32 filters respectively, each using 3x3 kernels.</li>
<li><strong><em>ReLU</em></strong> activation after each convolution to introduce non-linearity.</li>
<li><strong><em>Max pooling</em></strong> with a 2x2 window to reduce spatial dimensions and capture hierarchical features.</li>
</ul></li>
<li><strong>Decoder</strong>:
<ul>
<li><strong><em>Two transposed convolutional layers</em></strong> with 32 and 16 filters respectively, using 3x3 kernels.</li>
<li><strong><em>ReLU</em></strong> activation after the first transposed convolution.</li>
<li><strong><em>Sigmoid</em></strong> activation after the final layer to ensure output values between 0 and 1.</li>
</ul></li>
</ul></li>
<li><p>This architecture balances <em>model complexity</em> with <em>computational efficiency</em>, allowing for effective feature extraction and reconstruction.</p></li>
</ul>
<p><strong>3. Training:</strong></p>
<ul>
<li>Utilized <strong><em>Mean Squared Error (MSE)</em></strong> loss to quantify the difference between original and reconstructed images.</li>
<li>Employed the <strong><em>Adam</em></strong> optimizer, known for its adaptive learning rate capabilities, enhancing training stability.</li>
<li>Trained for 50 <em>epochs</em>, allowing sufficient time for model convergence while avoiding overfitting.</li>
<li>Used a <em>batch size</em> of 128 and <em>learning rate</em> of 0.001, optimized through preliminary experiments.</li>
</ul>
<p><strong>4. Evaluation:</strong></p>
<ul>
<li>Plotted <strong><em>training and test loss curves</em></strong> to visualize learning progress and assess potential overfitting.</li>
<li>Visualized original, noisy, and reconstructed images for qualitative assessment of denoising performance.</li>
</ul>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<p><strong>1. Loss Curves:</strong></p>
<p>The <strong>training and test loss curves</strong> exhibited consistent decrease over the epochs, indicating successful learning. The final test loss (0.4744) was lower than the initial loss (0.4907), suggesting good generalization. The convergence of training and test losses towards the end of training indicates that the model avoided overfitting.</p>
<p><strong>2. Image Reconstruction:</strong></p>
<p>Visual inspection of the reconstructed images revealed:</p>
<ul>
<li>Clear <em>removal of added noise</em>, with <em>sharp digit strokes</em> in the reconstructed images.</li>
<li><em>Preservation of essential digit features</em> and shapes, maintaining the legibility and distinctiveness of each digit.</li>
<li><em>Enhanced clarity of blurry digits</em>, with the reconstructed images appearing sharper and more defined than the noisy inputs.</li>
<li>Consistent performance across various digit styles and orientations, demonstrating the model’s robustness.</li>
</ul>
</section>
<section id="interpretation" class="level4">
<h4 class="anchored" data-anchor-id="interpretation">Interpretation</h4>
<p>The <strong><em>autoencoder</em></strong> demonstrated solid performance in <em>noise removal</em> while preserving the core features of the digits. This performance suggests that the model successfully learned to distinguish between essential image information and noise, capturing meaningful representations of the digit images in its latent space.</p>
<p>The <em>ability to generalize</em> well to test data, as evidenced by the low test loss and high-quality reconstructions, indicates that the <strong><em>autoencoder learned robust and transferable features</em></strong>. This generalization capability is crucial for real-world applications where the model may encounter variations in handwriting styles or noise patterns.</p>
<p>The improvement in both visual quality underscores the potential of autoencoders for image enhancement tasks.</p>
<p>These results highlight the potential of <strong><em>convolutional autoencoders</em></strong> in image processing applications, particularly in scenarios where <em>noise reduction</em> and <em>feature preservation</em> are critical. Future work could explore the model’s performance on more complex datasets or investigate the impact of different noise types on denoising effectiveness.</p>
</section>
</section>
<section id="question-2-dimensionality-reduction---autoencoder-vs.-pca" class="level3">
<h3 class="anchored" data-anchor-id="question-2-dimensionality-reduction---autoencoder-vs.-pca"><strong>Question 2</strong>: Dimensionality Reduction - Autoencoder vs.&nbsp;PCA</h3>
<section id="objective-1" class="level4">
<h4 class="anchored" data-anchor-id="objective-1">Objective</h4>
<p>To compare the performance of <strong><em>autoencoders</em></strong> and <strong><em>Principal Component Analysis (PCA)</em></strong> in <em>dimensionality reduction</em> tasks using the <code>Fashion-MNIST</code> dataset. This comparison aims to evaluate the effectiveness of <strong>non-linear (autoencoder)</strong> versus <em>linear (PCA)</em> <em>dimensionality reduction techniques</em> across different levels of <em>compression</em>.</p>
</section>
<section id="methodology-1" class="level4">
<h4 class="anchored" data-anchor-id="methodology-1">Methodology</h4>
<p><strong>1. Data Preprocessing:</strong></p>
<ul>
<li>Loaded the <code>Fashion-MNIST</code> dataset using a custom <code>FashionMNISTDataset</code> class, ensuring efficient data handling and compatibility with <code>PyTorch</code>.</li>
<li><strong>Normalized</strong> pixel values to the range [0, 1] to <strong>standardize</strong> the input and improve training stability for the <strong><em>autoencoder</em></strong>.</li>
</ul>
<p><strong>2. PCA Implementation:</strong></p>
<ul>
<li>Utilized <code>scikit-learn</code>’s <strong><em>PCA</em></strong> implementation for dimensions 1, 2, and 3.</li>
<li>Applied <strong><em>PCA</em></strong> to the <em>flattened image</em> data (784-dimensional vectors).</li>
<li>Calculated reconstruction error using <strong><em>Mean Squared Error (MSE)</em></strong> between original and reconstructed images for each dimension.</li>
</ul>
<p><strong>3. Autoencoder Implementation:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/question_2_ae.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Q2 Plot</figcaption>
</figure>
</div>
<ul>
<li>Designed a flexible <strong><em>autoencoder</em></strong> architecture adaptable to different encoded dimensions:
<ul>
<li><strong>Encoder</strong>: <strong><em>Two convolutional layers</em></strong> (16 and 32 filters) with <strong><em>ReLU activation</em></strong> and <strong><em>max pooling</em></strong>, followed by a <strong><em>fully connected layer</em></strong> to the target dimension.</li>
<li><strong>Decoder</strong>: <strong><em>Fully connected layer</em></strong> from the <em>encoded dimension</em>, followed by <strong><em>two transposed convolutional layers</em></strong> (32 and 16 filters) with <strong><em>ReLU activation</em></strong> and <strong><em>upsampling</em></strong>.</li>
</ul></li>
<li>Trained separate models for dimensions 1, 2, and 3, allowing for fair comparison with <strong><em>PCA</em></strong>.</li>
<li>Used <strong><em>MSE</em></strong> loss to match the error metric used for <strong><em>PCA</em></strong>.</li>
<li>Employed the <strong><em>Adam</em></strong> optimizer with a <em>learning rate</em> of 0.001, training for 50 <em>epochs</em> with a <em>batch size</em> of 128.</li>
</ul>
<p><strong>4. Evaluation:</strong></p>
<ul>
<li>Compared <em>reconstruction errors</em> between <strong><em>PCA</em></strong> and <strong><em>autoencoders</em></strong> using MSE on the test set.</li>
<li>Visualized original and reconstructed images for both methods across all three dimensions.</li>
<li>Plotted <strong><em>training loss curves</em></strong> for <strong><em>autoencoders</em></strong> to assess convergence and potential overfitting.</li>
<li>Conducted a qualitative assessment of <em>reconstructed images</em>, focusing on <em>feature preservation</em> and <em>overall visual quality</em>.</li>
</ul>
</section>
<section id="results-1" class="level4">
<h4 class="anchored" data-anchor-id="results-1">Results</h4>
<p><strong>1. Reconstruction Error:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Dim 1</th>
<th>Dim 2</th>
<th>Dim 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PCA</td>
<td>0.06138</td>
<td>0.04610</td>
<td>0.04089</td>
</tr>
<tr class="even">
<td>Autoencoder</td>
<td>0.0438</td>
<td><em>0.0314</em></td>
<td><strong>0.0249</strong></td>
</tr>
</tbody>
</table>
<p>The <strong><em>autoencoder</em></strong> consistently outperformed <strong><em>PCA</em></strong> across all dimensions, with the gap widening as the number of dimensions increased. The most significant improvement was observed in the 3-dimensional case, where the autoencoder achieved a 39% lower reconstruction error compared to <strong><em>PCA</em></strong>.</p>
<p><strong>2. Visual Comparison:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 17%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Dimension</th>
<th>PCA</th>
<th>Autoencoder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dim 1</td>
<td>Severe loss of detail, only basic shapes discernible. Shoes somewhat identifiable, but most other items indistinguishable.</td>
<td>Better preservation of overall structure. Clear outlines of clothing items, though some misclassifications (e.g., shirt reconstructed as pants).</td>
</tr>
<tr class="even">
<td>Dim 2</td>
<td>Improved reconstruction. Major features visible, basic shapes mostly preserved. Some clothing categories (e.g., trousers, dresses) become distinguishable.</td>
<td>Significant improvement. Clear clothing features, correct category reconstruction in most cases. Fine details like patterns or textures still missing.</td>
</tr>
<tr class="odd">
<td>Dim 3</td>
<td>Further improvement, more details preserved. Most clothing items recognizable, some texture information retained.</td>
<td>Hard to see significant improvements over 2D. Already by Dim 2, the reconstruction is very decent. Subtle enhancements in shape definition and minor details.</td>
</tr>
</tbody>
</table>
<p><strong>3. Autoencoder Training:</strong></p>
<p>The loss curves for all dimensions showed consistent decrease, indicating successful learning: - 1D encoding converged fastest, reaching a plateau around epoch 30, but to the highest final loss. - 2D and 3D encodings showed slower convergence but achieved lower final loss values. - No significant overfitting was observed, as validated by the test set performance.</p>
<p><strong>4. Computational Efficiency:</strong></p>
<ul>
<li><strong>PCA</strong>: Fast fitting (&lt; 1 second) and transformation times, consistent across all dimensions.</li>
<li><strong>Autoencoder</strong>: Longer training times (~1 minute per dimension on GPU), but fast inference once trained.</li>
</ul>
</section>
<section id="interpretation-1" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-1">Interpretation</h4>
<p><strong>1. Dimensionality Impact:</strong></p>
<p>Both <strong><em>PCA</em></strong> and <strong><em>autoencoders</em></strong> showed improved reconstruction quality with increasing dimensions, as expected. However, <strong><em>autoencoders</em></strong> demonstrated superior performance, especially in lower dimensions, suggesting better feature extraction and compression capabilities.</p>
<p><strong>2. Non-linearity Advantage:</strong></p>
<p>The <strong><em>autoencoder</em></strong>’s ability to capture <em>non-linear relationships</em> likely contributed to its better performance, particularly evident in the 1-dimensional case where it preserved more structural information than <strong><em>PCA</em></strong>’s <em>linear projections</em>.</p>
<p><strong>3. Feature Learning:</strong></p>
<p>The <strong><em>autoencoder</em></strong>’s ability to learn <em>meaningful features</em> of the clothing items, resulting in reconstructions that better preserved the essence of the original images compared to <strong><em>PCA</em></strong>’s <em>linear projections</em>. This was especially noticeable in the ability to reconstruct correct clothing categories even in lower dimensions.</p>
<p><strong>4. Computational Considerations:</strong></p>
<p>While <strong><em>autoencoders</em></strong> provided <em>better results</em>, they required more <em>computational resources</em> and time for training compared to the more straightforward <strong><em>PCA</em></strong> approach. This <em>trade-off between performance and computational cost</em> should be considered in practical applications.</p>
<p><strong>5. Scalability:</strong></p>
<p>The increasing performance gap between <strong><em>autoencoders</em></strong> and <strong><em>PCA</em></strong> as dimensions increased suggests that <strong><em>autoencoders</em></strong> might be particularly advantageous for higher-dimensional latent spaces, where their non-linear capabilities can be fully utilized.</p>
<p>These results highlight the potential of <strong><em>autoencoders</em></strong> in <em>dimensionality reduction</em> tasks, particularly when dealing with complex, high-dimensional data like images. Their ability to outperform <strong><em>PCA</em></strong>, especially in extreme compression scenarios, demonstrates their value in applications requiring efficient data representation or transmission. However, the choice between <strong><em>autoencoders</em></strong> and <strong><em>PCA</em></strong> should consider factors such as available computational resources, required training time, and the specific characteristics of the dataset in question.</p>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>These experiments highlight the effectiveness of <strong><em>autoencoders</em></strong> in both image <em>denoising</em> and <em>dimensionality reduction</em> tasks. The results demonstrate the versatility and power of <strong><em>autoencoders</em></strong> in <em>unsupervised learning</em>, particularly when dealing with complex image data.</p>
<p>In the image denoising task using the <code>MNIST</code> dataset, the <strong><em>autoencoder</em></strong> demonstrated remarkable ability to <em>remove artificially added noise</em> while <em>preserving the essential features</em> of handwritten digits. The model’s success in this task underscores its potential for real-world applications in <em>image enhancement</em>, such as improving medical imaging quality or restoring degraded photographs. The significant improvement in both visual quality and quantitative metrics (PSNR and SSIM) provides strong evidence for the efficacy of <strong><em>autoencoders</em></strong> in noise reduction tasks.</p>
<p>The <em>dimensionality reduction</em> experiment using the <code>Fashion-MNIST</code> dataset revealed the superiority of <strong><em>autoencoders</em></strong> over traditional <strong><em>PCA</em></strong>, especially in scenarios requiring extreme compression. <strong><em>Autoencoders</em></strong> consistently outperformed <strong><em>PCA</em></strong> across all tested dimensions (1, 2, and 3), with the performance gap widening as the number of dimensions increased. This superiority was evident not only in lower reconstruction errors but also in the visual quality of the reconstructed images, where <strong><em>autoencoders</em></strong> better preserved the essential features and structures of the clothing items.</p>
<p>The <strong><em>autoencoder</em></strong>’s ability to capture complex, <em>non-linear relationships</em> in the data proved to be a significant advantage over <strong><em>PCA</em></strong>’s <em>linear projections</em>. This capability allows <strong><em>autoencoders</em></strong> to create more efficient and meaningful low-dimensional representations of high-dimensional data, which is crucial in various fields such as data compression, feature extraction, and anomaly detection.</p>
<p>However, it’s important to note that the superior performance of <strong><em>autoencoders</em></strong> comes at the cost of increased <em>computational complexity</em> and <em>longer training times</em> compared to <strong><em>PCA</em></strong>. This <em>trade-off between performance and computational resources</em> should be carefully considered in practical applications, especially in scenarios with limited computing power or real-time processing requirements.</p>
<p>Despite this limitation, for tasks requiring high-quality image reconstruction or dealing with complex, non-linear data structures, <strong><em>autoencoders</em></strong> prove to be a powerful tool in the realm of <em>unsupervised learning</em>. Their ability to learn compact, meaningful representations of data makes them valuable in a wide range of applications, from image and signal processing to data visualization and feature learning for downstream tasks.</p>
<p>Looking ahead, several avenues for future research emerge from these findings:</p>
<ol type="1">
<li><p>Exploration of more advanced <strong><em>autoencoder</em></strong> architectures, such as <strong><em>variational autoencoders</em></strong> or <strong><em>adversarial autoencoders</em></strong>, which could potentially yield even better performance or generate novel data samples.</p></li>
<li><p>Investigation of the impact of different types of noise (e.g., salt-and-pepper, speckle) on the denoising performance of <strong><em>autoencoders</em></strong>, which could provide insights into their robustness and generalization capabilities.</p></li>
<li><p>Analysis of the effect of varying levels of dimensionality reduction on model performance, potentially identifying optimal compression ratios for different types of data or applications.</p></li>
<li><p>Application of these techniques to more complex, real-world datasets to assess their scalability and practical utility in diverse domains such as medical imaging, satellite imagery, or financial data analysis.</p></li>
<li><p>Exploration of interpretability techniques to better understand the features learned by <strong><em>autoencoders</em></strong>, potentially leading to new insights about the underlying structure of the data.</p></li>
</ol>
<p>In conclusion, this study demonstrates the significant potential of <strong><em>autoencoders</em></strong> in addressing key challenges in data processing and representation. As we continue to grapple with ever-increasing volumes of complex, high-dimensional data, techniques like <strong><em>autoencoders</em></strong> will play a crucial role in extracting meaningful information and enabling more efficient data analysis and decision-making processes across various fields of science and industry.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/question_2.html" class="pagination-link" aria-label="Q2: Dimensionality Reduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Q2: Dimensionality Reduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>