{"title":"Technical Report","markdown":{"headingText":"Technical Report","containsRefs":false,"markdown":"\n## Autoencoders for Image Enhancement and Dimensionality Reduction\n\n### Introduction\n\nThis report examines the application of ***autoencoders*** in two critical areas of image processing: *denoising* and *dimensionality reduction*. Our experiments utilize two well-known datasets to demonstrate the versatility and effectiveness of ***autoencoders*** in *unsupervised learning tasks*.\n\nIn the **first question**, we focus on image enhancement using the `MNIST` dataset of handwritten digits. We design and implement an ***autoencoder*** capable of removing artificially added noise from these images, effectively demonstrating the model's ability to learn and preserve essential features while filtering out unwanted information.\n\nThe **second question** compares the performance of ***autoencoders*** against ***Principal Component Analysis (PCA)*** in *dimensionality reduction* tasks. Using the `Fashion-MNIST` dataset, we evaluate both methods across three levels of reduced dimensionality: 1, 2, and 3 dimensions. This comparison provides insights into the strengths and limitations of autoencoders versus traditional linear dimensionality reduction techniques.\n\nThese questions are designed to address *key challenges* in data processing and visualization. By exploring autoencoders' capabilities in *noise reduction* and *data compression*, we aim to demonstrate their potential applications in fields ranging from image processing to *data transmission* and *storage optimization*.\n\nThe following sections detail our methodology, present the results of our questions, and discuss the implications of our findings for future applications of autoencoders in image processing and data analysis.\n\n### **Question 1**: Image Enhancement with Autoencoders\n\n#### Objective\n\nTo design and implement an ***autoencoder*** capable of *removing noise* from handwritten digit images from the `MNIST` dataset. This task aims to demonstrate the autoencoder's ability to *learn robust feature representations* and its potential for *image denoising* applications.\n\n#### Methodology\n\n**1. Data Preprocessing:**\n\n   - Loaded the `MNIST` dataset using a custom `MNISTDataset` class, ensuring efficient data handling and compatibility with `PyTorch`.\n   - Applied *normalization* with mean 0.1307 and standard deviation 0.3081, standardizing the input to improve training stability and convergence.\n   - Implemented a *noise addition function* to create noisy versions of images, simulating real-world image degradation. The function adds Gaussian noise with a standard deviation of 0.1, providing a challenging yet realistic *denoising task*.\n\n**2. Model Architecture:**\n\n![Q1 Plot](../images/question_1_ae.svg){width=100%}\n\n   - Designed a ***convolutional autoencoder*** with the following structure:\n\n     - **Encoder**: \n       - ***Two convolutional layers*** with 16 and 32 filters respectively, each using 3x3 kernels.\n       - ***ReLU*** activation after each convolution to introduce non-linearity.\n       - ***Max pooling*** with a 2x2 window to reduce spatial dimensions and capture hierarchical features.\n\n     - **Decoder**: \n       - ***Two transposed convolutional layers*** with 32 and 16 filters respectively, using 3x3 kernels.\n       - ***ReLU*** activation after the first transposed convolution.\n       - ***Sigmoid*** activation after the final layer to ensure output values between 0 and 1.\n   - This architecture balances *model complexity* with *computational efficiency*, allowing for effective feature extraction and reconstruction.\n\n**3. Training:**\n\n   - Utilized ***Mean Squared Error (MSE)*** loss to quantify the difference between original and reconstructed images.\n   - Employed the ***Adam*** optimizer, known for its adaptive learning rate capabilities, enhancing training stability.\n   - Trained for 50 *epochs*, allowing sufficient time for model convergence while avoiding overfitting.\n   - Used a *batch size* of 128 and *learning rate* of 0.001, optimized through preliminary experiments.\n\n**4. Evaluation:**\n\n   - Plotted ***training and test loss curves*** to visualize learning progress and assess potential overfitting.\n   - Visualized original, noisy, and reconstructed images for qualitative assessment of denoising performance.\n\n#### Results\n\n**1. Loss Curves:**\n\n   The **training and test loss curves** exhibited consistent decrease over the epochs, indicating successful learning. The final test loss (0.4744) was lower than the initial loss (0.4907), suggesting good generalization. The convergence of training and test losses towards the end of training indicates that the model avoided overfitting.\n\n**2. Image Reconstruction:**\n\n   Visual inspection of the reconstructed images revealed:\n\n   - Clear *removal of added noise*, with *sharp digit strokes* in the reconstructed images.\n   - *Preservation of essential digit features* and shapes, maintaining the legibility and distinctiveness of each digit.\n   - *Enhanced clarity of blurry digits*, with the reconstructed images appearing sharper and more defined than the noisy inputs.\n   - Consistent performance across various digit styles and orientations, demonstrating the model's robustness.\n\n#### Interpretation\n\nThe ***autoencoder*** demonstrated solid performance in *noise removal* while preserving the core features of the digits. This performance suggests that the model successfully learned to distinguish between essential image information and noise, capturing meaningful representations of the digit images in its latent space.\n\nThe *ability to generalize* well to test data, as evidenced by the low test loss and high-quality reconstructions, indicates that the ***autoencoder learned robust and transferable features***. This generalization capability is crucial for real-world applications where the model may encounter variations in handwriting styles or noise patterns.\n\nThe improvement in both visual quality underscores the potential of autoencoders for image enhancement tasks. \n\nThese results highlight the potential of ***convolutional autoencoders*** in image processing applications, particularly in scenarios where *noise reduction* and *feature preservation* are critical. Future work could explore the model's performance on more complex datasets or investigate the impact of different noise types on denoising effectiveness.\n\n### **Question 2**: Dimensionality Reduction - Autoencoder vs. PCA\n\n#### Objective\n\nTo compare the performance of ***autoencoders*** and ***Principal Component Analysis (PCA)*** in *dimensionality reduction* tasks using the `Fashion-MNIST` dataset. This comparison aims to evaluate the effectiveness of **non-linear (autoencoder)** versus *linear (PCA)* *dimensionality reduction techniques* across different levels of *compression*.\n\n#### Methodology\n\n**1. Data Preprocessing:**\n\n   - Loaded the `Fashion-MNIST` dataset using a custom `FashionMNISTDataset` class, ensuring efficient data handling and compatibility with `PyTorch`.\n   - **Normalized** pixel values to the range [0, 1] to **standardize** the input and improve training stability for the ***autoencoder***.\n\n**2. PCA Implementation:**\n\n   - Utilized `scikit-learn`'s ***PCA*** implementation for dimensions 1, 2, and 3.\n   - Applied ***PCA*** to the *flattened image* data (784-dimensional vectors).\n   - Calculated reconstruction error using ***Mean Squared Error (MSE)*** between original and reconstructed images for each dimension.\n\n**3. Autoencoder Implementation:**\n\n![Q2 Plot](../images/question_2_ae.svg){width=100%}\n\n   - Designed a flexible ***autoencoder*** architecture adaptable to different encoded dimensions:\n     - **Encoder**: ***Two convolutional layers*** (16 and 32 filters) with ***ReLU activation*** and ***max pooling***, followed by a ***fully connected layer*** to the target dimension.\n     - **Decoder**: ***Fully connected layer*** from the *encoded dimension*, followed by ***two transposed convolutional layers*** (32 and 16 filters) with ***ReLU activation*** and ***upsampling***.\n   - Trained separate models for dimensions 1, 2, and 3, allowing for fair comparison with ***PCA***.\n   - Used ***MSE*** loss to match the error metric used for ***PCA***.\n   - Employed the ***Adam*** optimizer with a *learning rate* of 0.001, training for 50 *epochs* with a *batch size* of 128.\n\n**4. Evaluation:**\n\n   - Compared *reconstruction errors* between ***PCA*** and ***autoencoders*** using MSE on the test set.\n   - Visualized original and reconstructed images for both methods across all three dimensions.\n   - Plotted ***training loss curves*** for ***autoencoders*** to assess convergence and potential overfitting.\n   - Conducted a qualitative assessment of *reconstructed images*, focusing on *feature preservation* and *overall visual quality*.\n\n#### Results\n\n**1. Reconstruction Error:**\n\n   | Method      | Dim 1   | Dim 2   | Dim 3   |\n   |-------------|---------|---------|---------|\n   | PCA         | 0.06138 | 0.04610 | 0.04089 |\n   | Autoencoder | 0.0438  | *0.0314*  | **0.0249**  |\n\n   The ***autoencoder*** consistently outperformed ***PCA*** across all dimensions, with the gap widening as the number of dimensions increased. The most significant improvement was observed in the 3-dimensional case, where the autoencoder achieved a 39% lower reconstruction error compared to ***PCA***.\n\n**2. Visual Comparison:**\n\n   | Dimension | PCA | Autoencoder |\n   |-----------|-----|-------------|\n   | Dim 1 | Severe loss of detail, only basic shapes discernible. Shoes somewhat identifiable, but most other items indistinguishable. | Better preservation of overall structure. Clear outlines of clothing items, though some misclassifications (e.g., shirt reconstructed as pants). |\n   | Dim 2 | Improved reconstruction. Major features visible, basic shapes mostly preserved. Some clothing categories (e.g., trousers, dresses) become distinguishable. | Significant improvement. Clear clothing features, correct category reconstruction in most cases. Fine details like patterns or textures still missing. |\n   | Dim 3 | Further improvement, more details preserved. Most clothing items recognizable, some texture information retained. | Hard to see significant improvements over 2D. Already by Dim 2, the reconstruction is very decent. Subtle enhancements in shape definition and minor details. |\n\n**3. Autoencoder Training:**\n\n   The loss curves for all dimensions showed consistent decrease, indicating successful learning:\n   - 1D encoding converged fastest, reaching a plateau around epoch 30, but to the highest final loss.\n   - 2D and 3D encodings showed slower convergence but achieved lower final loss values.\n   - No significant overfitting was observed, as validated by the test set performance.\n\n**4. Computational Efficiency:**\n\n   - **PCA**: Fast fitting (< 1 second) and transformation times, consistent across all dimensions.\n   - **Autoencoder**: Longer training times (~1 minute per dimension on GPU), but fast inference once trained.\n\n#### Interpretation\n\n**1. Dimensionality Impact:**\n\n   Both ***PCA*** and ***autoencoders*** showed improved reconstruction quality with increasing dimensions, as expected. However, ***autoencoders*** demonstrated superior performance, especially in lower dimensions, suggesting better feature extraction and compression capabilities.\n\n**2. Non-linearity Advantage:**\n\n   The ***autoencoder***'s ability to capture *non-linear relationships* likely contributed to its better performance, particularly evident in the 1-dimensional case where it preserved more structural information than ***PCA***'s *linear projections*.\n\n**3. Feature Learning:**\n\n   The ***autoencoder***'s ability to learn *meaningful features* of the clothing items, resulting in reconstructions that better preserved the essence of the original images compared to ***PCA***'s *linear projections*. This was especially noticeable in the ability to reconstruct correct clothing categories even in lower dimensions.\n\n**4. Computational Considerations:**\n\n   While ***autoencoders*** provided *better results*, they required more *computational resources* and time for training compared to the more straightforward ***PCA*** approach. This *trade-off between performance and computational cost* should be considered in practical applications.\n\n**5. Scalability:**\n\n   The increasing performance gap between ***autoencoders*** and ***PCA*** as dimensions increased suggests that ***autoencoders*** might be particularly advantageous for higher-dimensional latent spaces, where their non-linear capabilities can be fully utilized.\n\nThese results highlight the potential of ***autoencoders*** in *dimensionality reduction* tasks, particularly when dealing with complex, high-dimensional data like images. Their ability to outperform ***PCA***, especially in extreme compression scenarios, demonstrates their value in applications requiring efficient data representation or transmission. However, the choice between ***autoencoders*** and ***PCA*** should consider factors such as available computational resources, required training time, and the specific characteristics of the dataset in question.\n\n### Conclusion\n\nThese experiments highlight the effectiveness of ***autoencoders*** in both image *denoising* and *dimensionality reduction* tasks. The results demonstrate the versatility and power of ***autoencoders*** in *unsupervised learning*, particularly when dealing with complex image data.\n\nIn the image denoising task using the `MNIST` dataset, the ***autoencoder*** demonstrated remarkable ability to *remove artificially added noise* while *preserving the essential features* of handwritten digits. The model's success in this task underscores its potential for real-world applications in *image enhancement*, such as improving medical imaging quality or restoring degraded photographs. The significant improvement in both visual quality and quantitative metrics (PSNR and SSIM) provides strong evidence for the efficacy of ***autoencoders*** in noise reduction tasks.\n\nThe *dimensionality reduction* experiment using the `Fashion-MNIST` dataset revealed the superiority of ***autoencoders*** over traditional ***PCA***, especially in scenarios requiring extreme compression. ***Autoencoders*** consistently outperformed ***PCA*** across all tested dimensions (1, 2, and 3), with the performance gap widening as the number of dimensions increased. This superiority was evident not only in lower reconstruction errors but also in the visual quality of the reconstructed images, where ***autoencoders*** better preserved the essential features and structures of the clothing items.\n\nThe ***autoencoder***'s ability to capture complex, *non-linear relationships* in the data proved to be a significant advantage over ***PCA***'s *linear projections*. This capability allows ***autoencoders*** to create more efficient and meaningful low-dimensional representations of high-dimensional data, which is crucial in various fields such as data compression, feature extraction, and anomaly detection.\n\nHowever, it's important to note that the superior performance of ***autoencoders*** comes at the cost of increased *computational complexity* and *longer training times* compared to ***PCA***. This *trade-off between performance and computational resources* should be carefully considered in practical applications, especially in scenarios with limited computing power or real-time processing requirements.\n\nDespite this limitation, for tasks requiring high-quality image reconstruction or dealing with complex, non-linear data structures, ***autoencoders*** prove to be a powerful tool in the realm of *unsupervised learning*. Their ability to learn compact, meaningful representations of data makes them valuable in a wide range of applications, from image and signal processing to data visualization and feature learning for downstream tasks.\n\nLooking ahead, several avenues for future research emerge from these findings:\n\n1. Exploration of more advanced ***autoencoder*** architectures, such as ***variational autoencoders*** or ***adversarial autoencoders***, which could potentially yield even better performance or generate novel data samples.\n\n2. Investigation of the impact of different types of noise (e.g., salt-and-pepper, speckle) on the denoising performance of ***autoencoders***, which could provide insights into their robustness and generalization capabilities.\n\n3. Analysis of the effect of varying levels of dimensionality reduction on model performance, potentially identifying optimal compression ratios for different types of data or applications.\n\n4. Application of these techniques to more complex, real-world datasets to assess their scalability and practical utility in diverse domains such as medical imaging, satellite imagery, or financial data analysis.\n\n5. Exploration of interpretability techniques to better understand the features learned by ***autoencoders***, potentially leading to new insights about the underlying structure of the data.\n\nIn conclusion, this study demonstrates the significant potential of ***autoencoders*** in addressing key challenges in data processing and representation. As we continue to grapple with ever-increasing volumes of complex, high-dimensional data, techniques like ***autoencoders*** will play a crucial role in extracting meaningful information and enabling more efficient data analysis and decision-making processes across various fields of science and industry.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"output-file":"technical_report.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}