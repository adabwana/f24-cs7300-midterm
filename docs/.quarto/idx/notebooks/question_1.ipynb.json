{"title":"Q1: Image Enhancement","markdown":{"yaml":{"title":"Q1: Image Enhancement"},"headingText":"Autoencoder for MNIST Denoising","containsRefs":false,"markdown":"\n\n\n\n\nAutoencoders are neural networks designed to learn efficient data representations. They achieve this by compressing input data into a lower-dimensional space and then reconstructing it. In this project, we'll build and train an autoencoder to remove noise from handwritten digit images.\n\nThe process involves two main steps. First, the encoder compresses the input image. Then, the decoder attempts to reconstruct the original image from this compressed representation. By training on noisy images and comparing the output to clean originals, the autoencoder learns to distinguish between essential features and noise.\n\nWe'll use the MNIST dataset for this task. This dataset is widely used in machine learning and serves as an excellent starting point for image processing projects. Let's begin by implementing our autoencoder and exploring its denoising capabilities.\n\n\n\n## Data Preprocessing and Loading\n\nData preparation is a crucial step before training our autoencoder. This process involves several key components. First, we load the MNIST dataset, a standard collection of handwritten digits widely used in machine learning. Next, we apply normalization to the data. This step is essential for improving the model's performance and training stability.\n\nThe normalization process transforms the pixel values of the images. It adjusts them to have a mean of 0.1307 and a standard deviation of 0.3081. These specific values are derived from the statistics of the MNIST dataset (see [here](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457) or [here](https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset)). By normalizing the data, we ensure that all input features are on a similar scale. This standardization helps the model learn more effectively and converge faster during training.\n\nOur data loading process utilizes custom dataset and dataloader classes. These classes efficiently handle the reading and processing of the MNIST files. They also apply the necessary transformations to prepare the data for input into our autoencoder model.\n\nThis preprocessing step sets the foundation for our autoencoder training. It ensures that our model receives consistent, well-prepared data, which is crucial for achieving optimal results in our image denoising task.\n\n## Adding Noise to Images\n\nTo enhance the robustness of our autoencoder, we introduce artificial noise to the input images. This process serves two primary purposes:\n\n1. It challenges the model to learn essential features amidst distortions.\n2. It improves the model's generalization capabilities.\n\nBy training on noisy inputs, the autoencoder learns to differentiate between crucial image characteristics and random perturbations. This technique, often referred to as denoising, is a powerful method for unsupervised feature learning.\n\nThe noise addition function is implemented as follows:\n\n\nThis function adds Gaussian noise to the input image and clamps the resulting values to ensure they remain within the valid pixel range of [0, 1]. The standard deviation of 0.1 for the noise provides a balance between challenging the model and maintaining the image's core features.\n\n## Building the Autoencoder\n\nThe core of our project is the autoencoder neural network. This architecture consists of two primary components:\n\n1. The encoder: Compresses the input image into a lower-dimensional representation.\n2. The decoder: Reconstructs the original image from the compressed representation.\n\nThe autoencoder's structure is implemented as follows:\n\nKey aspects of this implementation:\n\n1. The encoder uses three convolutional layers with ReLU activation functions. Each layer increases the number of channels while maintaining spatial dimensions through padding.\n2. The decoder mirrors the encoder's structure, using transposed convolutions to upsample the data. The final layer uses a sigmoid activation to ensure output values are between 0 and 1.\n3. The forward method ties the encoding and decoding processes together, creating the full autoencoder pipeline.\n\nThis architecture allows the autoencoder to learn efficient representations of the input data, enabling it to denoise images effectively. The balance between the encoder and decoder is crucial for maintaining important features while removing noise.\n\n## Training the Autoencoder\n\nThe training phase is a critical component of our autoencoder project. During this process, the model learns to effectively remove noise from images. The training procedure involves the following key steps:\n\n1. We initialize the model, define the loss function (Mean Squared Error), and set up the optimizer (Adam) with our specified learning rate.\n2. We feed noisy images through the autoencoder.\n3. We compare the reconstructed outputs with the original, clean images.\n4. Adjusting the model's parameters to minimize the difference between reconstructions and originals\n\nWe implement this process over multiple epochs, gradually refining the model's ability to denoise images. The training loop is structured as follows:\n\n\nThis code iterates through the training data, performs forward and backward passes, and updates the model parameters. It also evaluates the model's performance on a separate test set to monitor for overfitting.\n\nTo visualize the training progress, we plot the training and test losses:\n\nThis visualization allows us to track the model's learning progress and identify any potential issues such as overfitting or underfitting.\n\nBy systematically exposing the autoencoder to noisy images and their clean counterparts, we enable it to learn robust representations that can effectively separate signal from noise in image data.\n\n\n## Visualizing the Results\n\nWe now proceed to evaluate the performance of our trained autoencoder. This process involves three key steps:\n\n1. Selecting a subset of test images\n2. Adding artificial noise to these images\n3. Passing the noisy images through our autoencoder for reconstruction\n\nThis evaluation allows us to visually assess the model's ability to denoise images effectively. We will display the original, noisy, and reconstructed images side by side for comparison.\n\nThe visualization process is implemented as follows:\n\n\nThis code generates a figure with three rows:\n\n1. Original test images\n2. Noisy versions of these images\n3. Reconstructed images produced by our autoencoder\n\nBy examining these results, we can gauge the effectiveness of our model in preserving essential features while removing noise. This visual comparison provides valuable insights into the autoencoder's performance and its potential applications in image denoising tasks.\n\n\n## Save the model\n","srcMarkdownNoYaml":"\n\n\n\n## Autoencoder for MNIST Denoising\n\nAutoencoders are neural networks designed to learn efficient data representations. They achieve this by compressing input data into a lower-dimensional space and then reconstructing it. In this project, we'll build and train an autoencoder to remove noise from handwritten digit images.\n\nThe process involves two main steps. First, the encoder compresses the input image. Then, the decoder attempts to reconstruct the original image from this compressed representation. By training on noisy images and comparing the output to clean originals, the autoencoder learns to distinguish between essential features and noise.\n\nWe'll use the MNIST dataset for this task. This dataset is widely used in machine learning and serves as an excellent starting point for image processing projects. Let's begin by implementing our autoencoder and exploring its denoising capabilities.\n\n\n\n## Data Preprocessing and Loading\n\nData preparation is a crucial step before training our autoencoder. This process involves several key components. First, we load the MNIST dataset, a standard collection of handwritten digits widely used in machine learning. Next, we apply normalization to the data. This step is essential for improving the model's performance and training stability.\n\nThe normalization process transforms the pixel values of the images. It adjusts them to have a mean of 0.1307 and a standard deviation of 0.3081. These specific values are derived from the statistics of the MNIST dataset (see [here](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457) or [here](https://stackoverflow.com/questions/63746182/correct-way-of-normalizing-and-scaling-the-mnist-dataset)). By normalizing the data, we ensure that all input features are on a similar scale. This standardization helps the model learn more effectively and converge faster during training.\n\nOur data loading process utilizes custom dataset and dataloader classes. These classes efficiently handle the reading and processing of the MNIST files. They also apply the necessary transformations to prepare the data for input into our autoencoder model.\n\nThis preprocessing step sets the foundation for our autoencoder training. It ensures that our model receives consistent, well-prepared data, which is crucial for achieving optimal results in our image denoising task.\n\n## Adding Noise to Images\n\nTo enhance the robustness of our autoencoder, we introduce artificial noise to the input images. This process serves two primary purposes:\n\n1. It challenges the model to learn essential features amidst distortions.\n2. It improves the model's generalization capabilities.\n\nBy training on noisy inputs, the autoencoder learns to differentiate between crucial image characteristics and random perturbations. This technique, often referred to as denoising, is a powerful method for unsupervised feature learning.\n\nThe noise addition function is implemented as follows:\n\n\nThis function adds Gaussian noise to the input image and clamps the resulting values to ensure they remain within the valid pixel range of [0, 1]. The standard deviation of 0.1 for the noise provides a balance between challenging the model and maintaining the image's core features.\n\n## Building the Autoencoder\n\nThe core of our project is the autoencoder neural network. This architecture consists of two primary components:\n\n1. The encoder: Compresses the input image into a lower-dimensional representation.\n2. The decoder: Reconstructs the original image from the compressed representation.\n\nThe autoencoder's structure is implemented as follows:\n\nKey aspects of this implementation:\n\n1. The encoder uses three convolutional layers with ReLU activation functions. Each layer increases the number of channels while maintaining spatial dimensions through padding.\n2. The decoder mirrors the encoder's structure, using transposed convolutions to upsample the data. The final layer uses a sigmoid activation to ensure output values are between 0 and 1.\n3. The forward method ties the encoding and decoding processes together, creating the full autoencoder pipeline.\n\nThis architecture allows the autoencoder to learn efficient representations of the input data, enabling it to denoise images effectively. The balance between the encoder and decoder is crucial for maintaining important features while removing noise.\n\n## Training the Autoencoder\n\nThe training phase is a critical component of our autoencoder project. During this process, the model learns to effectively remove noise from images. The training procedure involves the following key steps:\n\n1. We initialize the model, define the loss function (Mean Squared Error), and set up the optimizer (Adam) with our specified learning rate.\n2. We feed noisy images through the autoencoder.\n3. We compare the reconstructed outputs with the original, clean images.\n4. Adjusting the model's parameters to minimize the difference between reconstructions and originals\n\nWe implement this process over multiple epochs, gradually refining the model's ability to denoise images. The training loop is structured as follows:\n\n\nThis code iterates through the training data, performs forward and backward passes, and updates the model parameters. It also evaluates the model's performance on a separate test set to monitor for overfitting.\n\nTo visualize the training progress, we plot the training and test losses:\n\nThis visualization allows us to track the model's learning progress and identify any potential issues such as overfitting or underfitting.\n\nBy systematically exposing the autoencoder to noisy images and their clean counterparts, we enable it to learn robust representations that can effectively separate signal from noise in image data.\n\n\n## Visualizing the Results\n\nWe now proceed to evaluate the performance of our trained autoencoder. This process involves three key steps:\n\n1. Selecting a subset of test images\n2. Adding artificial noise to these images\n3. Passing the noisy images through our autoencoder for reconstruction\n\nThis evaluation allows us to visually assess the model's ability to denoise images effectively. We will display the original, noisy, and reconstructed images side by side for comparison.\n\nThe visualization process is implemented as follows:\n\n\nThis code generates a figure with three rows:\n\n1. Original test images\n2. Noisy versions of these images\n3. Reconstructed images produced by our autoencoder\n\nBy examining these results, we can gauge the effectiveness of our model in preserving essential features while removing noise. This visual comparison provides valuable insights into the autoencoder's performance and its potential applications in image denoising tasks.\n\n\n## Save the model\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"output-file":"question_1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Q1: Image Enhancement"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}