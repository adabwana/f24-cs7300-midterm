{"title":"Q2: Dimensionality Reduction","markdown":{"yaml":{"title":"Q2: Dimensionality Reduction"},"headingText":"Dimensionality Reduction: Autoencoder vs. PCA","containsRefs":false,"markdown":"\n\n\n\n\nThis notebook explores two dimensionality reduction techniques: Autoencoders and Principal Component Analysis (PCA). We'll apply these methods to the Fashion-MNIST dataset, a collection of 70,000 grayscale images of clothing items.\n\nOur objectives are to:\n\n1. Implement an autoencoder using PyTorch\n2. Apply PCA using scikit-learn\n3. Compare the performance of both methods across various reduced dimensions\n\nDimensionality reduction is crucial in data science for several reasons:\n\n- It can accelerate machine learning algorithms\n- It helps in visualizing high-dimensional data\n- It can reduce storage requirements for large datasets\n\nBy the end of this notebook, you'll understand the strengths and limitations of autoencoders and PCA in the context of image data compression.\n\nLet's begin by setting up our environment and loading the necessary libraries:\n\n\n\n## Data Preprocessing and Loading\n\nFor the Fashion-MNIST dataset, we implement a custom dataset class to efficiently handle data loading and preprocessing. This approach ensures optimal performance and flexibility in our data pipeline.\n\nKey components of our data preprocessing:\n\n1. Custom Dataset Class: We define a `FashionMNISTDataset` class that inherits from PyTorch's `Dataset`. This class manages the reading of image and label files, and applies necessary transformations.\n2. Data Transformation: We use a simple lambda function to convert our data into PyTorch tensors and add a channel dimension.\n3. File Path Management: We construct absolute paths to our data files, ensuring compatibility across different system environments.\n4. Data Loaders: We create separate data loaders for training and testing sets, enabling efficient batching and shuffling of data.\n\nThe implementation of these components can be found in the following code blocks:\n\n\nThis preprocessing pipeline transforms our raw Fashion-MNIST data into a format suitable for training our autoencoder and applying PCA. It ensures that our data is properly normalized, batched, and ready for efficient processing by our models.\n\n## Autoencoder Model\n\nWe now define our autoencoder architecture, which is crucial for dimensionality reduction. The model consists of two main components:\n\n1. Encoder: Compresses the input data into a lower-dimensional representation.\n2. Decoder: Reconstructs the original data from the compressed representation.\n\nOur autoencoder is implemented as a PyTorch module with the following structure:\n\n\nKey aspects of this implementation:\n\n1. The encoder uses two convolutional layers followed by a fully connected layer to compress the input.\n2. The decoder mirrors this structure, using a fully connected layer followed by two transposed convolutional layers to reconstruct the image.\n3. We use ReLU activation functions for intermediate layers and a sigmoid activation for the final output layer.\n4. The `encoded_dim` parameter allows us to flexibly adjust the dimensionality of the compressed representation.\n\nThis architecture enables our autoencoder to learn efficient, lower-dimensional representations of the Fashion-MNIST images. By varying the `encoded_dim`, we can explore different levels of compression and their impact on reconstruction quality.\n\n## Training Function\n\nTo streamline our autoencoder training process, we define a dedicated function. This function encapsulates the entire training loop, including both training and validation steps. Here's an overview of its key components:\n\n1. Epoch iteration: The function runs for a specified number of epochs.\n2. Training phase: In each epoch, it processes the training data, computes loss, and updates model parameters.\n3. Validation phase: After each training epoch, it evaluates the model on the test set.\n4. Loss tracking: It records both training and test losses for later analysis.\n\nThe implementation of this function is as follows:\n\nThis function takes the model, data loaders, number of epochs, optimizer, and loss criterion as inputs. It returns the training and test loss histories, which we can use to visualize the learning progress.\nKey aspects of this implementation:\n\n1. We use `model.train()` and `model.eval()` to switch between training and evaluation modes.\n2. The function handles both autoencoder training (reconstruction) and dimensionality reduction (encoding).\n3. We use `torch.no_grad()` during evaluation to disable gradient computation and save memory.\n\nBy encapsulating the training process in a function, we enhance code reusability and make it easier to train autoencoders with different configurations or on different datasets.\n\n## Visualization Function\n\nTo effectively evaluate the performance of our dimensionality reduction techniques, we implement a visualization function. This function allows us to compare original images with their reconstructed counterparts visually. Key aspects of this function include:\n\n1. Side-by-side display of original and reconstructed images\n2. Clear labeling of the dimensionality reduction method and encoded dimension\n3. Efficient use of matplotlib for creating the visualization\n\nThe implementation of this function is as follows:\n\n\nThis function takes four parameters:\n\n- `original`: A batch of original images\n- `reconstructed`: A batch of reconstructed images\n- `method`: The dimensionality reduction method used (e.g., \"PCA\" or \"Autoencoder\")\n- `dim`: The dimension of the encoded representation\n\nThe function creates a figure with two rows:\n\n1. The top row displays the original images\n2. The bottom row shows the corresponding reconstructed images\n\nBy using this visualization function, we can:\n\n- Quickly assess the quality of reconstructions\n- Compare the performance of different dimensionality reduction techniques\n- Observe how the encoded dimension affects reconstruction quality\n\nThis visual feedback is crucial for understanding the trade-offs between compression and reconstruction fidelity in our dimensionality reduction experiments.\n\n\n## Main Execution\n\nIn this section, we conduct a comprehensive comparison between PCA and our autoencoder for dimensionality reduction. We evaluate both methods across multiple reduced dimensions to assess their performance and reconstruction capabilities.\n\nOur experimental procedure involves the following steps:\n\n1. Iterate through different reduced dimensions (1, 2, and 3).\n2. For each dimension:\n    - Apply PCA to the dataset and measure reconstruction error.\n    - Train an autoencoder with the corresponding encoded dimension.\n    - Visualize and compare the results of both methods.\n\nThis approach allows us to:\n\n- Assess how the reduced dimension affects reconstruction quality for each method.\n- Compare the computational efficiency of PCA versus autoencoder training.\n- Visualize the reconstructed images to qualitatively evaluate performance.\n\nThe implementation of our main execution loop is as follows:\n\n\nKey aspects of this implementation:\n\n1. We use a loop to iterate through different dimensions, allowing for easy comparison.\n2. For PCA, we utilize scikit-learn's implementation, applying it to flattened image data.\n3. For the autoencoder, we create a new model for each dimension, train it, and evaluate its performance.\n4. We use our previously defined visualization function to display results for both methods.\n\nBy structuring our experiments this way, we can efficiently compare PCA and autoencoder performance across various dimensions. This comprehensive analysis provides insights into the strengths and weaknesses of each method in the context of image data compression and reconstruction.\n\n\n## Save the model\n","srcMarkdownNoYaml":"\n\n\n\n## Dimensionality Reduction: Autoencoder vs. PCA\n\nThis notebook explores two dimensionality reduction techniques: Autoencoders and Principal Component Analysis (PCA). We'll apply these methods to the Fashion-MNIST dataset, a collection of 70,000 grayscale images of clothing items.\n\nOur objectives are to:\n\n1. Implement an autoencoder using PyTorch\n2. Apply PCA using scikit-learn\n3. Compare the performance of both methods across various reduced dimensions\n\nDimensionality reduction is crucial in data science for several reasons:\n\n- It can accelerate machine learning algorithms\n- It helps in visualizing high-dimensional data\n- It can reduce storage requirements for large datasets\n\nBy the end of this notebook, you'll understand the strengths and limitations of autoencoders and PCA in the context of image data compression.\n\nLet's begin by setting up our environment and loading the necessary libraries:\n\n\n\n## Data Preprocessing and Loading\n\nFor the Fashion-MNIST dataset, we implement a custom dataset class to efficiently handle data loading and preprocessing. This approach ensures optimal performance and flexibility in our data pipeline.\n\nKey components of our data preprocessing:\n\n1. Custom Dataset Class: We define a `FashionMNISTDataset` class that inherits from PyTorch's `Dataset`. This class manages the reading of image and label files, and applies necessary transformations.\n2. Data Transformation: We use a simple lambda function to convert our data into PyTorch tensors and add a channel dimension.\n3. File Path Management: We construct absolute paths to our data files, ensuring compatibility across different system environments.\n4. Data Loaders: We create separate data loaders for training and testing sets, enabling efficient batching and shuffling of data.\n\nThe implementation of these components can be found in the following code blocks:\n\n\nThis preprocessing pipeline transforms our raw Fashion-MNIST data into a format suitable for training our autoencoder and applying PCA. It ensures that our data is properly normalized, batched, and ready for efficient processing by our models.\n\n## Autoencoder Model\n\nWe now define our autoencoder architecture, which is crucial for dimensionality reduction. The model consists of two main components:\n\n1. Encoder: Compresses the input data into a lower-dimensional representation.\n2. Decoder: Reconstructs the original data from the compressed representation.\n\nOur autoencoder is implemented as a PyTorch module with the following structure:\n\n\nKey aspects of this implementation:\n\n1. The encoder uses two convolutional layers followed by a fully connected layer to compress the input.\n2. The decoder mirrors this structure, using a fully connected layer followed by two transposed convolutional layers to reconstruct the image.\n3. We use ReLU activation functions for intermediate layers and a sigmoid activation for the final output layer.\n4. The `encoded_dim` parameter allows us to flexibly adjust the dimensionality of the compressed representation.\n\nThis architecture enables our autoencoder to learn efficient, lower-dimensional representations of the Fashion-MNIST images. By varying the `encoded_dim`, we can explore different levels of compression and their impact on reconstruction quality.\n\n## Training Function\n\nTo streamline our autoencoder training process, we define a dedicated function. This function encapsulates the entire training loop, including both training and validation steps. Here's an overview of its key components:\n\n1. Epoch iteration: The function runs for a specified number of epochs.\n2. Training phase: In each epoch, it processes the training data, computes loss, and updates model parameters.\n3. Validation phase: After each training epoch, it evaluates the model on the test set.\n4. Loss tracking: It records both training and test losses for later analysis.\n\nThe implementation of this function is as follows:\n\nThis function takes the model, data loaders, number of epochs, optimizer, and loss criterion as inputs. It returns the training and test loss histories, which we can use to visualize the learning progress.\nKey aspects of this implementation:\n\n1. We use `model.train()` and `model.eval()` to switch between training and evaluation modes.\n2. The function handles both autoencoder training (reconstruction) and dimensionality reduction (encoding).\n3. We use `torch.no_grad()` during evaluation to disable gradient computation and save memory.\n\nBy encapsulating the training process in a function, we enhance code reusability and make it easier to train autoencoders with different configurations or on different datasets.\n\n## Visualization Function\n\nTo effectively evaluate the performance of our dimensionality reduction techniques, we implement a visualization function. This function allows us to compare original images with their reconstructed counterparts visually. Key aspects of this function include:\n\n1. Side-by-side display of original and reconstructed images\n2. Clear labeling of the dimensionality reduction method and encoded dimension\n3. Efficient use of matplotlib for creating the visualization\n\nThe implementation of this function is as follows:\n\n\nThis function takes four parameters:\n\n- `original`: A batch of original images\n- `reconstructed`: A batch of reconstructed images\n- `method`: The dimensionality reduction method used (e.g., \"PCA\" or \"Autoencoder\")\n- `dim`: The dimension of the encoded representation\n\nThe function creates a figure with two rows:\n\n1. The top row displays the original images\n2. The bottom row shows the corresponding reconstructed images\n\nBy using this visualization function, we can:\n\n- Quickly assess the quality of reconstructions\n- Compare the performance of different dimensionality reduction techniques\n- Observe how the encoded dimension affects reconstruction quality\n\nThis visual feedback is crucial for understanding the trade-offs between compression and reconstruction fidelity in our dimensionality reduction experiments.\n\n\n## Main Execution\n\nIn this section, we conduct a comprehensive comparison between PCA and our autoencoder for dimensionality reduction. We evaluate both methods across multiple reduced dimensions to assess their performance and reconstruction capabilities.\n\nOur experimental procedure involves the following steps:\n\n1. Iterate through different reduced dimensions (1, 2, and 3).\n2. For each dimension:\n    - Apply PCA to the dataset and measure reconstruction error.\n    - Train an autoencoder with the corresponding encoded dimension.\n    - Visualize and compare the results of both methods.\n\nThis approach allows us to:\n\n- Assess how the reduced dimension affects reconstruction quality for each method.\n- Compare the computational efficiency of PCA versus autoencoder training.\n- Visualize the reconstructed images to qualitatively evaluate performance.\n\nThe implementation of our main execution loop is as follows:\n\n\nKey aspects of this implementation:\n\n1. We use a loop to iterate through different dimensions, allowing for easy comparison.\n2. For PCA, we utilize scikit-learn's implementation, applying it to flattened image data.\n3. For the autoencoder, we create a new model for each dimension, train it, and evaluate its performance.\n4. We use our previously defined visualization function to display results for both methods.\n\nBy structuring our experiments this way, we can efficiently compare PCA and autoencoder performance across various dimensions. This comprehensive analysis provides insights into the strengths and weaknesses of each method in the context of image data compression and reconstruction.\n\n\n## Save the model\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":4,"number-sections":false,"output-file":"question_2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Q2: Dimensionality Reduction"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}